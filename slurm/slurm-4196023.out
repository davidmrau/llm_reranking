
Tue Oct 17 10:03:18 2023       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A100-SXM4-40GB          On  | 00000000:32:00.0 Off |                  Off |
| N/A   29C    P0              49W / 400W |      4MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA A100-SXM4-40GB          On  | 00000000:E3:00.0 Off |                  Off |
| N/A   29C    P0              49W / 400W |      4MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
/gpfs/home3/drau/llm_rankers/venv/lib/python3.9/site-packages/torch/cuda/__init__.py:107: UserWarning: CUDA initialization: CUDA driver initialization failed, you might not have a CUDA gpu. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)
  return torch._C._cuda_getDeviceCount() > 0
- - - - - - - - - -  Quantization and Flash Attention  2.0 not used! - - - - - - - - - - 
Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]Loading checkpoint shards:   7%|▋         | 1/15 [00:04<00:58,  4.18s/it]Loading checkpoint shards:  13%|█▎        | 2/15 [00:08<00:58,  4.50s/it]Loading checkpoint shards:  20%|██        | 3/15 [00:16<01:13,  6.14s/it]Loading checkpoint shards:  27%|██▋       | 4/15 [00:25<01:17,  7.09s/it]Loading checkpoint shards:  33%|███▎      | 5/15 [00:34<01:16,  7.67s/it]Loading checkpoint shards:  40%|████      | 6/15 [00:42<01:11,  7.93s/it]Loading checkpoint shards:  47%|████▋     | 7/15 [00:50<01:03,  7.98s/it]Loading checkpoint shards:  53%|█████▎    | 8/15 [00:59<00:57,  8.20s/it]Loading checkpoint shards:  60%|██████    | 9/15 [01:09<00:52,  8.77s/it]Loading checkpoint shards:  67%|██████▋   | 10/15 [01:21<00:48,  9.79s/it]Loading checkpoint shards:  73%|███████▎  | 11/15 [01:33<00:41, 10.31s/it]Loading checkpoint shards:  80%|████████  | 12/15 [01:43<00:31, 10.45s/it]Loading checkpoint shards:  87%|████████▋ | 13/15 [01:56<00:22, 11.04s/it]slurm/trec_dl20_llama_class.sh: line 3: 586567 Killed                  python3 rerank_class.py trec_dl20 meta-llama/Llama-2-70b-hf
/gpfs/home3/drau/llm_rankers/venv/lib/python3.9/site-packages/torch/cuda/__init__.py:107: UserWarning: CUDA initialization: CUDA driver initialization failed, you might not have a CUDA gpu. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)
  return torch._C._cuda_getDeviceCount() > 0
- - - - - - - - - -  Quantization and Flash Attention  2.0 not used! - - - - - - - - - - 
Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]Loading checkpoint shards:   7%|▋         | 1/15 [00:07<01:48,  7.76s/it]Loading checkpoint shards:  13%|█▎        | 2/15 [00:15<01:42,  7.86s/it]Loading checkpoint shards:  20%|██        | 3/15 [00:23<01:34,  7.88s/it]Loading checkpoint shards:  27%|██▋       | 4/15 [00:31<01:27,  7.96s/it]Loading checkpoint shards:  33%|███▎      | 5/15 [00:39<01:20,  8.06s/it]Loading checkpoint shards:  40%|████      | 6/15 [00:48<01:13,  8.19s/it]Loading checkpoint shards:  47%|████▋     | 7/15 [00:56<01:04,  8.03s/it]Loading checkpoint shards:  53%|█████▎    | 8/15 [01:03<00:55,  7.99s/it]Loading checkpoint shards:  60%|██████    | 9/15 [01:13<00:51,  8.51s/it]Loading checkpoint shards:  67%|██████▋   | 10/15 [01:25<00:47,  9.53s/it]Loading checkpoint shards:  73%|███████▎  | 11/15 [01:35<00:39,  9.82s/it]Loading checkpoint shards:  80%|████████  | 12/15 [01:46<00:30, 10.03s/it]Loading checkpoint shards:  87%|████████▋ | 13/15 [01:58<00:21, 10.62s/it]slurm/trec_dl20_llama_class.sh: line 4: 587459 Killed                  python3 rerank_class.py trec_dl20 meta-llama/Llama-2-70b-chat-hf
slurmstepd: error: Detected 2 oom_kill events in StepId=4196023.batch. Some of the step tasks have been OOM Killed.

JOB STATISTICS
==============
Job ID: 4196023
Cluster: snellius
User/Group: drau/drau
State: OUT_OF_MEMORY (exit code 0)
Nodes: 1
Cores per node: 36
CPU Utilized: 00:38:18
CPU Efficiency: 20.66% of 03:05:24 core-walltime
Job Wall-clock time: 00:05:09
Memory Utilized: 206.88 GB
Memory Efficiency: 86.20% of 240.00 GB

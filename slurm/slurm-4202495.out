
Tue Oct 17 18:40:45 2023       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A100-SXM4-40GB          On  | 00000000:31:00.0 Off |                  Off |
| N/A   28C    P0              49W / 400W |      4MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:15<00:15, 15.22s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:20<00:00,  9.46s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:20<00:00, 10.32s/it]
Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.
You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32001. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc
trec_dl19
class_reranking_llama_bz_1/beir_bm25_runs_top100_trec_dl19_meta-llama_Llama-2-7b-hf
DatasetDict({
    queries: Dataset({
        features: ['_id', 'text', 'title'],
        num_rows: 43
    })
    corpus: Dataset({
        features: ['_id', 'text', 'title'],
        num_rows: 5482
    })
}) queries
  0%|          | 0/43 [00:00<?, ?it/s]100%|██████████| 43/43 [00:00<00:00, 31127.90it/s]
DatasetDict({
    queries: Dataset({
        features: ['_id', 'text', 'title'],
        num_rows: 43
    })
    corpus: Dataset({
        features: ['_id', 'text', 'title'],
        num_rows: 5482
    })
}) corpus
  0%|          | 0/5482 [00:00<?, ?it/s] 63%|██████▎   | 3455/5482 [00:00<00:00, 34547.23it/s]100%|██████████| 5482/5482 [00:00<00:00, 34571.73it/s]
  0%|          | 0/135 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/gpfs/home3/drau/llm_rankers/venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/gpfs/home3/drau/llm_rankers/venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
  1%|          | 1/135 [00:01<04:16,  1.91s/it]  1%|▏         | 2/135 [00:02<02:35,  1.17s/it]  2%|▏         | 3/135 [00:03<02:02,  1.08it/s]  3%|▎         | 4/135 [00:03<01:44,  1.25it/s]  4%|▎         | 5/135 [00:04<01:31,  1.41it/s]  4%|▍         | 6/135 [00:05<01:31,  1.41it/s]  5%|▌         | 7/135 [00:05<01:26,  1.48it/s]  6%|▌         | 8/135 [00:06<01:22,  1.53it/s]  7%|▋         | 9/135 [00:06<01:20,  1.57it/s]  7%|▋         | 10/135 [00:07<01:15,  1.65it/s]  8%|▊         | 11/135 [00:08<01:14,  1.66it/s]  9%|▉         | 12/135 [00:08<01:11,  1.71it/s] 10%|▉         | 13/135 [00:09<01:14,  1.63it/s] 10%|█         | 14/135 [00:09<01:16,  1.59it/s] 11%|█         | 15/135 [00:10<01:13,  1.63it/s] 12%|█▏        | 16/135 [00:11<01:16,  1.56it/s] 13%|█▎        | 17/135 [00:11<01:11,  1.66it/s] 13%|█▎        | 18/135 [00:12<01:09,  1.68it/s] 14%|█▍        | 19/135 [00:12<01:06,  1.75it/s] 15%|█▍        | 20/135 [00:13<01:06,  1.73it/s] 16%|█▌        | 21/135 [00:14<01:12,  1.58it/s] 16%|█▋        | 22/135 [00:14<01:11,  1.58it/s] 17%|█▋        | 23/135 [00:15<01:13,  1.52it/s] 18%|█▊        | 24/135 [00:16<01:20,  1.38it/s] 19%|█▊        | 25/135 [00:17<01:17,  1.42it/s] 19%|█▉        | 26/135 [00:17<01:19,  1.37it/s] 20%|██        | 27/135 [00:18<01:18,  1.38it/s] 21%|██        | 28/135 [00:19<01:13,  1.45it/s] 21%|██▏       | 29/135 [00:19<01:09,  1.53it/s] 22%|██▏       | 30/135 [00:20<01:03,  1.65it/s] 23%|██▎       | 31/135 [00:20<01:01,  1.70it/s] 24%|██▎       | 32/135 [00:21<01:02,  1.66it/s] 24%|██▍       | 33/135 [00:21<01:01,  1.66it/s] 25%|██▌       | 34/135 [00:22<01:04,  1.58it/s] 26%|██▌       | 35/135 [00:23<01:03,  1.57it/s] 27%|██▋       | 36/135 [00:23<00:57,  1.71it/s] 27%|██▋       | 37/135 [00:24<00:57,  1.69it/s] 28%|██▊       | 38/135 [00:25<00:57,  1.67it/s] 29%|██▉       | 39/135 [00:25<00:57,  1.67it/s] 30%|██▉       | 40/135 [00:26<00:57,  1.66it/s] 30%|███       | 41/135 [00:26<00:59,  1.57it/s] 31%|███       | 42/135 [00:27<00:59,  1.56it/s] 32%|███▏      | 43/135 [00:28<00:57,  1.60it/s] 33%|███▎      | 44/135 [00:28<00:56,  1.61it/s] 33%|███▎      | 45/135 [00:29<00:55,  1.64it/s] 34%|███▍      | 46/135 [00:29<00:54,  1.63it/s] 35%|███▍      | 47/135 [00:30<00:51,  1.71it/s] 36%|███▌      | 48/135 [00:31<00:50,  1.72it/s] 36%|███▋      | 49/135 [00:31<00:51,  1.67it/s] 37%|███▋      | 50/135 [00:32<00:54,  1.55it/s] 38%|███▊      | 51/135 [00:33<00:55,  1.50it/s] 39%|███▊      | 52/135 [00:33<00:52,  1.59it/s] 39%|███▉      | 53/135 [00:34<00:49,  1.65it/s] 40%|████      | 54/135 [00:34<00:47,  1.69it/s] 41%|████      | 55/135 [00:35<00:47,  1.69it/s] 41%|████▏     | 56/135 [00:36<00:49,  1.58it/s] 42%|████▏     | 57/135 [00:36<00:47,  1.64it/s] 43%|████▎     | 58/135 [00:37<00:44,  1.73it/s] 44%|████▎     | 59/135 [00:37<00:42,  1.79it/s] 44%|████▍     | 60/135 [00:38<00:44,  1.69it/s] 45%|████▌     | 61/135 [00:39<00:44,  1.65it/s] 46%|████▌     | 62/135 [00:39<00:47,  1.53it/s] 47%|████▋     | 63/135 [00:40<00:45,  1.57it/s] 47%|████▋     | 64/135 [00:40<00:42,  1.66it/s] 48%|████▊     | 65/135 [00:41<00:39,  1.77it/s] 49%|████▉     | 66/135 [00:42<00:42,  1.62it/s] 50%|████▉     | 67/135 [00:42<00:43,  1.55it/s] 50%|█████     | 68/135 [00:43<00:44,  1.51it/s] 51%|█████     | 69/135 [00:44<00:42,  1.56it/s] 52%|█████▏    | 70/135 [00:44<00:40,  1.61it/s] 53%|█████▎    | 71/135 [00:45<00:38,  1.67it/s] 53%|█████▎    | 72/135 [00:45<00:38,  1.65it/s] 54%|█████▍    | 73/135 [00:46<00:36,  1.68it/s] 55%|█████▍    | 74/135 [00:47<00:35,  1.70it/s] 56%|█████▌    | 75/135 [00:47<00:35,  1.71it/s] 56%|█████▋    | 76/135 [00:48<00:35,  1.65it/s] 57%|█████▋    | 77/135 [00:48<00:35,  1.64it/s] 58%|█████▊    | 78/135 [00:49<00:34,  1.63it/s] 59%|█████▊    | 79/135 [00:50<00:33,  1.69it/s] 59%|█████▉    | 80/135 [00:50<00:31,  1.74it/s] 60%|██████    | 81/135 [00:51<00:33,  1.61it/s] 61%|██████    | 82/135 [00:51<00:32,  1.64it/s] 61%|██████▏   | 83/135 [00:52<00:31,  1.67it/s] 62%|██████▏   | 84/135 [00:52<00:29,  1.75it/s] 63%|██████▎   | 85/135 [00:53<00:32,  1.55it/s] 64%|██████▎   | 86/135 [00:54<00:32,  1.52it/s] 64%|██████▍   | 87/135 [00:55<00:30,  1.59it/s] 65%|██████▌   | 88/135 [00:55<00:29,  1.58it/s] 66%|██████▌   | 89/135 [00:56<00:28,  1.60it/s] 67%|██████▋   | 90/135 [00:56<00:27,  1.64it/s] 67%|██████▋   | 91/135 [00:57<00:28,  1.54it/s] 68%|██████▊   | 92/135 [00:58<00:26,  1.62it/s] 69%|██████▉   | 93/135 [00:58<00:26,  1.60it/s] 70%|██████▉   | 94/135 [00:59<00:25,  1.60it/s] 70%|███████   | 95/135 [01:00<00:24,  1.63it/s] 71%|███████   | 96/135 [01:00<00:25,  1.50it/s] 72%|███████▏  | 97/135 [01:01<00:24,  1.58it/s] 73%|███████▎  | 98/135 [01:01<00:23,  1.61it/s] 73%|███████▎  | 99/135 [01:02<00:22,  1.62it/s] 74%|███████▍  | 100/135 [01:03<00:23,  1.52it/s] 75%|███████▍  | 101/135 [01:03<00:21,  1.59it/s] 76%|███████▌  | 102/135 [01:04<00:21,  1.55it/s] 76%|███████▋  | 103/135 [01:05<00:19,  1.68it/s] 77%|███████▋  | 104/135 [01:05<00:18,  1.64it/s] 78%|███████▊  | 105/135 [01:06<00:18,  1.61it/s] 79%|███████▊  | 106/135 [01:06<00:18,  1.60it/s] 79%|███████▉  | 107/135 [01:07<00:17,  1.57it/s] 80%|████████  | 108/135 [01:08<00:16,  1.63it/s] 81%|████████  | 109/135 [01:08<00:15,  1.64it/s] 81%|████████▏ | 110/135 [01:09<00:15,  1.60it/s] 82%|████████▏ | 111/135 [01:10<00:14,  1.64it/s] 83%|████████▎ | 112/135 [01:10<00:13,  1.76it/s] 84%|████████▎ | 113/135 [01:11<00:12,  1.81it/s] 84%|████████▍ | 114/135 [01:11<00:12,  1.71it/s] 85%|████████▌ | 115/135 [01:12<00:11,  1.67it/s] 86%|████████▌ | 116/135 [01:12<00:11,  1.65it/s] 87%|████████▋ | 117/135 [01:13<00:10,  1.72it/s] 87%|████████▋ | 118/135 [01:14<00:10,  1.67it/s] 88%|████████▊ | 119/135 [01:14<00:09,  1.60it/s] 89%|████████▉ | 120/135 [01:15<00:08,  1.72it/s] 90%|████████▉ | 121/135 [01:15<00:08,  1.71it/s] 90%|█████████ | 122/135 [01:16<00:07,  1.64it/s] 91%|█████████ | 123/135 [01:17<00:07,  1.63it/s] 92%|█████████▏| 124/135 [01:17<00:06,  1.64it/s] 93%|█████████▎| 125/135 [01:18<00:06,  1.57it/s] 93%|█████████▎| 126/135 [01:19<00:05,  1.56it/s] 94%|█████████▍| 127/135 [01:19<00:05,  1.50it/s] 95%|█████████▍| 128/135 [01:20<00:04,  1.56it/s] 96%|█████████▌| 129/135 [01:20<00:03,  1.60it/s] 96%|█████████▋| 130/135 [01:21<00:03,  1.44it/s] 97%|█████████▋| 131/135 [01:22<00:02,  1.54it/s] 98%|█████████▊| 132/135 [01:23<00:01,  1.52it/s] 99%|█████████▊| 133/135 [01:23<00:01,  1.57it/s] 99%|█████████▉| 134/135 [01:24<00:00,  1.66it/s]100%|██████████| 135/135 [01:24<00:00,  2.04it/s]100%|██████████| 135/135 [01:24<00:00,  1.60it/s]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
ndcg_cut_10 0.2261
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:15<00:15, 15.43s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:20<00:00,  9.62s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:20<00:00, 10.50s/it]
Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.
You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32001. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc
trec_dl19
class_reranking_llama_bz_1/beir_bm25_runs_top100_trec_dl19_meta-llama_Llama-2-7b-chat-hf
DatasetDict({
    queries: Dataset({
        features: ['_id', 'text', 'title'],
        num_rows: 43
    })
    corpus: Dataset({
        features: ['_id', 'text', 'title'],
        num_rows: 5482
    })
}) queries
  0%|          | 0/43 [00:00<?, ?it/s]100%|██████████| 43/43 [00:00<00:00, 30951.62it/s]
DatasetDict({
    queries: Dataset({
        features: ['_id', 'text', 'title'],
        num_rows: 43
    })
    corpus: Dataset({
        features: ['_id', 'text', 'title'],
        num_rows: 5482
    })
}) corpus
  0%|          | 0/5482 [00:00<?, ?it/s] 65%|██████▌   | 3571/5482 [00:00<00:00, 35708.33it/s]100%|██████████| 5482/5482 [00:00<00:00, 35787.32it/s]
  0%|          | 0/135 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/gpfs/home3/drau/llm_rankers/venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/gpfs/home3/drau/llm_rankers/venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
  1%|          | 1/135 [00:01<03:15,  1.46s/it]  1%|▏         | 2/135 [00:02<02:10,  1.02it/s]  2%|▏         | 3/135 [00:02<01:48,  1.22it/s]  3%|▎         | 4/135 [00:03<01:36,  1.35it/s]  4%|▎         | 5/135 [00:03<01:26,  1.50it/s]  4%|▍         | 6/135 [00:04<01:27,  1.47it/s]  5%|▌         | 7/135 [00:05<01:23,  1.52it/s]  6%|▌         | 8/135 [00:05<01:21,  1.56it/s]  7%|▋         | 9/135 [00:06<01:19,  1.59it/s]  7%|▋         | 10/135 [00:06<01:15,  1.66it/s]  8%|▊         | 11/135 [00:07<01:14,  1.68it/s]  9%|▉         | 12/135 [00:08<01:11,  1.72it/s] 10%|▉         | 13/135 [00:08<01:14,  1.64it/s] 10%|█         | 14/135 [00:09<01:15,  1.60it/s] 11%|█         | 15/135 [00:09<01:13,  1.64it/s] 12%|█▏        | 16/135 [00:10<01:16,  1.56it/s] 13%|█▎        | 17/135 [00:11<01:11,  1.65it/s] 13%|█▎        | 18/135 [00:11<01:09,  1.68it/s] 14%|█▍        | 19/135 [00:12<01:06,  1.75it/s] 15%|█▍        | 20/135 [00:12<01:06,  1.73it/s] 16%|█▌        | 21/135 [00:13<01:12,  1.58it/s] 16%|█▋        | 22/135 [00:14<01:11,  1.58it/s] 17%|█▋        | 23/135 [00:15<01:13,  1.52it/s] 18%|█▊        | 24/135 [00:15<01:20,  1.38it/s] 19%|█▊        | 25/135 [00:16<01:17,  1.42it/s] 19%|█▉        | 26/135 [00:17<01:19,  1.37it/s] 20%|██        | 27/135 [00:18<01:18,  1.38it/s] 21%|██        | 28/135 [00:18<01:13,  1.45it/s] 21%|██▏       | 29/135 [00:19<01:09,  1.53it/s] 22%|██▏       | 30/135 [00:19<01:03,  1.65it/s] 23%|██▎       | 31/135 [00:20<01:01,  1.70it/s] 24%|██▎       | 32/135 [00:20<01:02,  1.66it/s] 24%|██▍       | 33/135 [00:21<01:01,  1.65it/s] 25%|██▌       | 34/135 [00:22<01:04,  1.58it/s] 26%|██▌       | 35/135 [00:22<01:03,  1.57it/s] 27%|██▋       | 36/135 [00:23<00:58,  1.70it/s] 27%|██▋       | 37/135 [00:23<00:57,  1.69it/s] 28%|██▊       | 38/135 [00:24<00:57,  1.68it/s] 29%|██▉       | 39/135 [00:25<00:57,  1.68it/s] 30%|██▉       | 40/135 [00:25<00:57,  1.66it/s] 30%|███       | 41/135 [00:26<00:59,  1.57it/s] 31%|███       | 42/135 [00:27<00:59,  1.56it/s] 32%|███▏      | 43/135 [00:27<00:57,  1.61it/s] 33%|███▎      | 44/135 [00:28<00:56,  1.61it/s] 33%|███▎      | 45/135 [00:28<00:55,  1.63it/s] 34%|███▍      | 46/135 [00:29<00:54,  1.63it/s] 35%|███▍      | 47/135 [00:30<00:51,  1.71it/s] 36%|███▌      | 48/135 [00:30<00:50,  1.72it/s] 36%|███▋      | 49/135 [00:31<00:51,  1.67it/s] 37%|███▋      | 50/135 [00:32<00:54,  1.56it/s] 38%|███▊      | 51/135 [00:32<00:55,  1.50it/s] 39%|███▊      | 52/135 [00:33<00:52,  1.59it/s] 39%|███▉      | 53/135 [00:33<00:49,  1.65it/s] 40%|████      | 54/135 [00:34<00:47,  1.69it/s] 41%|████      | 55/135 [00:34<00:47,  1.69it/s] 41%|████▏     | 56/135 [00:35<00:50,  1.58it/s] 42%|████▏     | 57/135 [00:36<00:47,  1.64it/s] 43%|████▎     | 58/135 [00:36<00:44,  1.73it/s] 44%|████▎     | 59/135 [00:37<00:42,  1.79it/s] 44%|████▍     | 60/135 [00:37<00:44,  1.69it/s] 45%|████▌     | 61/135 [00:38<00:44,  1.65it/s] 46%|████▌     | 62/135 [00:39<00:47,  1.53it/s] 47%|████▋     | 63/135 [00:39<00:45,  1.57it/s] 47%|████▋     | 64/135 [00:40<00:42,  1.66it/s] 48%|████▊     | 65/135 [00:40<00:39,  1.76it/s] 49%|████▉     | 66/135 [00:41<00:42,  1.62it/s] 50%|████▉     | 67/135 [00:42<00:43,  1.55it/s] 50%|█████     | 68/135 [00:43<00:44,  1.50it/s] 51%|█████     | 69/135 [00:43<00:42,  1.55it/s] 52%|█████▏    | 70/135 [00:44<00:40,  1.61it/s] 53%|█████▎    | 71/135 [00:44<00:38,  1.67it/s] 53%|█████▎    | 72/135 [00:45<00:38,  1.64it/s] 54%|█████▍    | 73/135 [00:46<00:37,  1.67it/s] 55%|█████▍    | 74/135 [00:46<00:35,  1.70it/s] 56%|█████▌    | 75/135 [00:47<00:35,  1.71it/s] 56%|█████▋    | 76/135 [00:47<00:36,  1.64it/s] 57%|█████▋    | 77/135 [00:48<00:35,  1.64it/s] 58%|█████▊    | 78/135 [00:49<00:34,  1.63it/s] 59%|█████▊    | 79/135 [00:49<00:33,  1.69it/s] 59%|█████▉    | 80/135 [00:50<00:31,  1.73it/s] 60%|██████    | 81/135 [00:50<00:33,  1.60it/s] 61%|██████    | 82/135 [00:51<00:32,  1.65it/s] 61%|██████▏   | 83/135 [00:52<00:31,  1.67it/s] 62%|██████▏   | 84/135 [00:52<00:29,  1.74it/s] 63%|██████▎   | 85/135 [00:53<00:32,  1.55it/s] 64%|██████▎   | 86/135 [00:54<00:32,  1.52it/s] 64%|██████▍   | 87/135 [00:54<00:30,  1.59it/s] 65%|██████▌   | 88/135 [00:55<00:29,  1.58it/s] 66%|██████▌   | 89/135 [00:55<00:28,  1.60it/s] 67%|██████▋   | 90/135 [00:56<00:27,  1.64it/s] 67%|██████▋   | 91/135 [00:57<00:28,  1.54it/s] 68%|██████▊   | 92/135 [00:57<00:26,  1.62it/s] 69%|██████▉   | 93/135 [00:58<00:26,  1.60it/s] 70%|██████▉   | 94/135 [00:58<00:25,  1.60it/s] 70%|███████   | 95/135 [00:59<00:24,  1.63it/s] 71%|███████   | 96/135 [01:00<00:26,  1.50it/s] 72%|███████▏  | 97/135 [01:00<00:24,  1.57it/s] 73%|███████▎  | 98/135 [01:01<00:22,  1.61it/s] 73%|███████▎  | 99/135 [01:02<00:22,  1.62it/s] 74%|███████▍  | 100/135 [01:02<00:23,  1.51it/s] 75%|███████▍  | 101/135 [01:03<00:21,  1.59it/s] 76%|███████▌  | 102/135 [01:04<00:21,  1.55it/s] 76%|███████▋  | 103/135 [01:04<00:19,  1.68it/s] 77%|███████▋  | 104/135 [01:05<00:18,  1.63it/s] 78%|███████▊  | 105/135 [01:05<00:18,  1.61it/s] 79%|███████▊  | 106/135 [01:06<00:18,  1.60it/s] 79%|███████▉  | 107/135 [01:07<00:17,  1.57it/s] 80%|████████  | 108/135 [01:07<00:16,  1.63it/s] 81%|████████  | 109/135 [01:08<00:15,  1.63it/s] 81%|████████▏ | 110/135 [01:09<00:15,  1.60it/s] 82%|████████▏ | 111/135 [01:09<00:14,  1.64it/s] 83%|████████▎ | 112/135 [01:10<00:13,  1.75it/s] 84%|████████▎ | 113/135 [01:10<00:12,  1.80it/s] 84%|████████▍ | 114/135 [01:11<00:12,  1.71it/s] 85%|████████▌ | 115/135 [01:11<00:11,  1.67it/s] 86%|████████▌ | 116/135 [01:12<00:11,  1.64it/s] 87%|████████▋ | 117/135 [01:13<00:10,  1.71it/s] 87%|████████▋ | 118/135 [01:13<00:10,  1.66it/s] 88%|████████▊ | 119/135 [01:14<00:09,  1.60it/s] 89%|████████▉ | 120/135 [01:14<00:08,  1.72it/s] 90%|████████▉ | 121/135 [01:15<00:08,  1.70it/s] 90%|█████████ | 122/135 [01:16<00:07,  1.64it/s] 91%|█████████ | 123/135 [01:16<00:07,  1.62it/s] 92%|█████████▏| 124/135 [01:17<00:06,  1.63it/s] 93%|█████████▎| 125/135 [01:18<00:06,  1.56it/s] 93%|█████████▎| 126/135 [01:18<00:05,  1.56it/s] 94%|█████████▍| 127/135 [01:19<00:05,  1.49it/s] 95%|█████████▍| 128/135 [01:20<00:04,  1.56it/s] 96%|█████████▌| 129/135 [01:20<00:03,  1.60it/s] 96%|█████████▋| 130/135 [01:21<00:03,  1.44it/s] 97%|█████████▋| 131/135 [01:22<00:02,  1.54it/s] 98%|█████████▊| 132/135 [01:22<00:01,  1.52it/s] 99%|█████████▊| 133/135 [01:23<00:01,  1.56it/s] 99%|█████████▉| 134/135 [01:23<00:00,  1.65it/s]100%|██████████| 135/135 [01:24<00:00,  2.04it/s]100%|██████████| 135/135 [01:24<00:00,  1.61it/s]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
ndcg_cut_10 0.4006
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:14<00:28, 14.32s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:28<00:14, 14.19s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:37<00:00, 11.86s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:37<00:00, 12.50s/it]
Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.
You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32001. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc
trec_dl19
class_reranking_llama_bz_1/beir_bm25_runs_top100_trec_dl19_meta-llama_Llama-2-13b-hf
DatasetDict({
    queries: Dataset({
        features: ['_id', 'text', 'title'],
        num_rows: 43
    })
    corpus: Dataset({
        features: ['_id', 'text', 'title'],
        num_rows: 5482
    })
}) queries
  0%|          | 0/43 [00:00<?, ?it/s]100%|██████████| 43/43 [00:00<00:00, 30698.74it/s]
DatasetDict({
    queries: Dataset({
        features: ['_id', 'text', 'title'],
        num_rows: 43
    })
    corpus: Dataset({
        features: ['_id', 'text', 'title'],
        num_rows: 5482
    })
}) corpus
  0%|          | 0/5482 [00:00<?, ?it/s] 64%|██████▍   | 3524/5482 [00:00<00:00, 35236.00it/s]100%|██████████| 5482/5482 [00:00<00:00, 35322.00it/s]
  0%|          | 0/135 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/gpfs/home3/drau/llm_rankers/venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/gpfs/home3/drau/llm_rankers/venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
  1%|          | 1/135 [00:01<03:55,  1.76s/it]  1%|▏         | 2/135 [00:02<03:02,  1.37s/it]  2%|▏         | 3/135 [00:03<02:44,  1.24s/it]  3%|▎         | 4/135 [00:05<02:34,  1.18s/it]  4%|▎         | 5/135 [00:05<02:21,  1.09s/it]  4%|▍         | 6/135 [00:07<02:25,  1.13s/it]  5%|▌         | 7/135 [00:08<02:22,  1.11s/it]  6%|▌         | 8/135 [00:09<02:19,  1.10s/it]  7%|▋         | 9/135 [00:10<02:15,  1.08s/it]  7%|▋         | 10/135 [00:11<02:08,  1.03s/it]  8%|▊         | 11/135 [00:12<02:07,  1.03s/it]  9%|▉         | 12/135 [00:13<02:03,  1.00s/it] 10%|▉         | 13/135 [00:14<02:08,  1.06s/it] 10%|█         | 14/135 [00:15<02:11,  1.08s/it] 11%|█         | 15/135 [00:16<02:07,  1.06s/it] 12%|█▏        | 16/135 [00:17<02:12,  1.11s/it] 13%|█▎        | 17/135 [00:18<02:03,  1.05s/it] 13%|█▎        | 18/135 [00:19<01:59,  1.03s/it] 14%|█▍        | 19/135 [00:20<01:54,  1.01it/s] 15%|█▍        | 20/135 [00:21<01:55,  1.00s/it] 16%|█▌        | 21/135 [00:22<02:05,  1.10s/it] 16%|█▋        | 22/135 [00:24<02:04,  1.10s/it] 17%|█▋        | 23/135 [00:25<02:08,  1.15s/it] 18%|█▊        | 24/135 [00:26<02:19,  1.26s/it] 19%|█▊        | 25/135 [00:27<02:14,  1.23s/it] 19%|█▉        | 26/135 [00:29<02:18,  1.27s/it] 20%|██        | 27/135 [00:30<02:17,  1.27s/it] 21%|██        | 28/135 [00:31<02:08,  1.20s/it] 21%|██▏       | 29/135 [00:32<02:00,  1.14s/it] 22%|██▏       | 30/135 [00:33<01:50,  1.05s/it] 23%|██▎       | 31/135 [00:34<01:45,  1.01s/it] 24%|██▎       | 32/135 [00:35<01:47,  1.05s/it] 24%|██▍       | 33/135 [00:36<01:46,  1.04s/it] 25%|██▌       | 34/135 [00:37<01:50,  1.09s/it] 26%|██▌       | 35/135 [00:38<01:49,  1.10s/it] 27%|██▋       | 36/135 [00:39<01:39,  1.01s/it] 27%|██▋       | 37/135 [00:40<01:39,  1.02s/it] 28%|██▊       | 38/135 [00:41<01:39,  1.03s/it] 29%|██▉       | 39/135 [00:42<01:38,  1.03s/it] 30%|██▉       | 40/135 [00:43<01:39,  1.04s/it] 30%|███       | 41/135 [00:45<01:43,  1.10s/it] 31%|███       | 42/135 [00:46<01:43,  1.12s/it] 32%|███▏      | 43/135 [00:47<01:39,  1.08s/it] 33%|███▎      | 44/135 [00:48<01:38,  1.08s/it] 33%|███▎      | 45/135 [00:49<01:35,  1.07s/it] 34%|███▍      | 46/135 [00:50<01:34,  1.06s/it] 35%|███▍      | 47/135 [00:51<01:29,  1.01s/it] 36%|███▌      | 48/135 [00:52<01:27,  1.01s/it] 36%|███▋      | 49/135 [00:53<01:29,  1.04s/it] 37%|███▋      | 50/135 [00:54<01:34,  1.12s/it] 38%|███▊      | 51/135 [00:56<01:37,  1.16s/it] 39%|███▊      | 52/135 [00:56<01:30,  1.09s/it] 39%|███▉      | 53/135 [00:57<01:25,  1.05s/it] 40%|████      | 54/135 [00:58<01:22,  1.02s/it] 41%|████      | 55/135 [00:59<01:21,  1.02s/it] 41%|████▏     | 56/135 [01:01<01:26,  1.09s/it] 42%|████▏     | 57/135 [01:02<01:21,  1.05s/it] 43%|████▎     | 58/135 [01:02<01:16,  1.01it/s] 44%|████▎     | 59/135 [01:03<01:13,  1.04it/s] 44%|████▍     | 60/135 [01:04<01:16,  1.02s/it] 45%|████▌     | 61/135 [01:06<01:17,  1.05s/it] 46%|████▌     | 62/135 [01:07<01:22,  1.13s/it] 47%|████▋     | 63/135 [01:08<01:19,  1.10s/it] 47%|████▋     | 64/135 [01:09<01:14,  1.04s/it] 48%|████▊     | 65/135 [01:10<01:08,  1.02it/s] 49%|████▉     | 66/135 [01:11<01:13,  1.07s/it] 50%|████▉     | 67/135 [01:12<01:16,  1.13s/it] 50%|█████     | 68/135 [01:13<01:17,  1.15s/it] 51%|█████     | 69/135 [01:14<01:13,  1.12s/it] 52%|█████▏    | 70/135 [01:15<01:09,  1.07s/it] 53%|█████▎    | 71/135 [01:16<01:06,  1.04s/it] 53%|█████▎    | 72/135 [01:18<01:06,  1.05s/it] 54%|█████▍    | 73/135 [01:19<01:04,  1.04s/it] 55%|█████▍    | 74/135 [01:19<01:02,  1.02s/it] 56%|█████▌    | 75/135 [01:20<01:00,  1.01s/it] 56%|█████▋    | 76/135 [01:22<01:02,  1.06s/it] 57%|█████▋    | 77/135 [01:23<01:01,  1.07s/it] 58%|█████▊    | 78/135 [01:24<01:00,  1.07s/it] 59%|█████▊    | 79/135 [01:25<00:57,  1.03s/it] 59%|█████▉    | 80/135 [01:26<00:54,  1.00it/s] 60%|██████    | 81/135 [01:27<00:58,  1.08s/it] 61%|██████    | 82/135 [01:28<00:55,  1.05s/it] 61%|██████▏   | 83/135 [01:29<00:53,  1.03s/it] 62%|██████▏   | 84/135 [01:30<00:50,  1.01it/s] 63%|██████▎   | 85/135 [01:31<00:55,  1.12s/it] 64%|██████▎   | 86/135 [01:32<00:55,  1.14s/it] 64%|██████▍   | 87/135 [01:33<00:52,  1.09s/it] 65%|██████▌   | 88/135 [01:35<00:51,  1.10s/it] 66%|██████▌   | 89/135 [01:36<00:50,  1.09s/it] 67%|██████▋   | 90/135 [01:37<00:47,  1.06s/it] 67%|██████▋   | 91/135 [01:38<00:49,  1.13s/it] 68%|██████▊   | 92/135 [01:39<00:46,  1.07s/it] 69%|██████▉   | 93/135 [01:40<00:45,  1.09s/it] 70%|██████▉   | 94/135 [01:41<00:44,  1.09s/it] 70%|███████   | 95/135 [01:42<00:42,  1.07s/it] 71%|███████   | 96/135 [01:43<00:45,  1.16s/it] 72%|███████▏  | 97/135 [01:44<00:41,  1.10s/it] 73%|███████▎  | 98/135 [01:45<00:39,  1.08s/it] 73%|███████▎  | 99/135 [01:46<00:38,  1.08s/it] 74%|███████▍  | 100/135 [01:48<00:40,  1.15s/it] 75%|███████▍  | 101/135 [01:49<00:37,  1.09s/it] 76%|███████▌  | 102/135 [01:50<00:36,  1.12s/it] 76%|███████▋  | 103/135 [01:51<00:33,  1.03s/it] 77%|███████▋  | 104/135 [01:52<00:32,  1.06s/it] 78%|███████▊  | 105/135 [01:53<00:32,  1.08s/it] 79%|███████▊  | 106/135 [01:54<00:31,  1.09s/it] 79%|███████▉  | 107/135 [01:55<00:31,  1.11s/it] 80%|████████  | 108/135 [01:56<00:28,  1.06s/it] 81%|████████  | 109/135 [01:57<00:27,  1.07s/it] 81%|████████▏ | 110/135 [01:58<00:27,  1.09s/it] 82%|████████▏ | 111/135 [01:59<00:25,  1.06s/it] 83%|████████▎ | 112/135 [02:00<00:22,  1.01it/s] 84%|████████▎ | 113/135 [02:01<00:21,  1.04it/s] 84%|████████▍ | 114/135 [02:02<00:21,  1.01s/it] 85%|████████▌ | 115/135 [02:03<00:20,  1.04s/it] 86%|████████▌ | 116/135 [02:04<00:20,  1.06s/it] 87%|████████▋ | 117/135 [02:05<00:18,  1.02s/it] 87%|████████▋ | 118/135 [02:07<00:17,  1.04s/it] 88%|████████▊ | 119/135 [02:08<00:17,  1.08s/it] 89%|████████▉ | 120/135 [02:09<00:15,  1.01s/it] 90%|████████▉ | 121/135 [02:10<00:14,  1.02s/it] 90%|█████████ | 122/135 [02:11<00:13,  1.05s/it] 91%|█████████ | 123/135 [02:12<00:12,  1.07s/it] 92%|█████████▏| 124/135 [02:13<00:11,  1.06s/it] 93%|█████████▎| 125/135 [02:14<00:11,  1.11s/it] 93%|█████████▎| 126/135 [02:15<00:10,  1.11s/it] 94%|█████████▍| 127/135 [02:16<00:09,  1.16s/it] 95%|█████████▍| 128/135 [02:17<00:07,  1.11s/it] 96%|█████████▌| 129/135 [02:18<00:06,  1.09s/it] 96%|█████████▋| 130/135 [02:20<00:06,  1.20s/it] 97%|█████████▋| 131/135 [02:21<00:04,  1.12s/it] 98%|█████████▊| 132/135 [02:22<00:03,  1.14s/it] 99%|█████████▊| 133/135 [02:23<00:02,  1.11s/it] 99%|█████████▉| 134/135 [02:24<00:01,  1.05s/it]100%|██████████| 135/135 [02:24<00:00,  1.18it/s]100%|██████████| 135/135 [02:24<00:00,  1.07s/it]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
ndcg_cut_10 0.3199
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:14<00:28, 14.25s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:28<00:14, 14.12s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:37<00:00, 11.82s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:37<00:00, 12.46s/it]
Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.
You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32001. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc
trec_dl19
class_reranking_llama_bz_1/beir_bm25_runs_top100_trec_dl19_meta-llama_Llama-2-13b-chat-hf
DatasetDict({
    queries: Dataset({
        features: ['_id', 'text', 'title'],
        num_rows: 43
    })
    corpus: Dataset({
        features: ['_id', 'text', 'title'],
        num_rows: 5482
    })
}) queries
  0%|          | 0/43 [00:00<?, ?it/s]100%|██████████| 43/43 [00:00<00:00, 31052.87it/s]
DatasetDict({
    queries: Dataset({
        features: ['_id', 'text', 'title'],
        num_rows: 43
    })
    corpus: Dataset({
        features: ['_id', 'text', 'title'],
        num_rows: 5482
    })
}) corpus
  0%|          | 0/5482 [00:00<?, ?it/s] 64%|██████▍   | 3506/5482 [00:00<00:00, 35057.02it/s]100%|██████████| 5482/5482 [00:00<00:00, 35144.81it/s]
  0%|          | 0/135 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/gpfs/home3/drau/llm_rankers/venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/gpfs/home3/drau/llm_rankers/venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
  1%|          | 1/135 [00:01<03:57,  1.77s/it]  1%|▏         | 2/135 [00:02<03:02,  1.37s/it]  2%|▏         | 3/135 [00:03<02:44,  1.25s/it]  3%|▎         | 4/135 [00:05<02:34,  1.18s/it]  4%|▎         | 5/135 [00:05<02:22,  1.09s/it]  4%|▍         | 6/135 [00:07<02:26,  1.13s/it]  5%|▌         | 7/135 [00:08<02:22,  1.11s/it]  6%|▌         | 8/135 [00:09<02:19,  1.10s/it]  7%|▋         | 9/135 [00:10<02:16,  1.08s/it]  7%|▋         | 10/135 [00:11<02:08,  1.03s/it]  8%|▊         | 11/135 [00:12<02:07,  1.03s/it]  9%|▉         | 12/135 [00:13<02:03,  1.01s/it] 10%|▉         | 13/135 [00:14<02:09,  1.06s/it] 10%|█         | 14/135 [00:15<02:10,  1.08s/it] 11%|█         | 15/135 [00:16<02:07,  1.06s/it] 12%|█▏        | 16/135 [00:17<02:12,  1.12s/it] 13%|█▎        | 17/135 [00:18<02:04,  1.05s/it] 13%|█▎        | 18/135 [00:19<01:59,  1.02s/it] 14%|█▍        | 19/135 [00:20<01:54,  1.01it/s] 15%|█▍        | 20/135 [00:21<01:55,  1.00s/it] 16%|█▌        | 21/135 [00:22<02:05,  1.10s/it] 16%|█▋        | 22/135 [00:24<02:03,  1.10s/it] 17%|█▋        | 23/135 [00:25<02:08,  1.15s/it] 18%|█▊        | 24/135 [00:26<02:19,  1.26s/it] 19%|█▊        | 25/135 [00:28<02:15,  1.23s/it] 19%|█▉        | 26/135 [00:29<02:18,  1.27s/it] 20%|██        | 27/135 [00:30<02:16,  1.27s/it] 21%|██        | 28/135 [00:31<02:08,  1.20s/it] 21%|██▏       | 29/135 [00:32<02:01,  1.14s/it] 22%|██▏       | 30/135 [00:33<01:50,  1.05s/it] 23%|██▎       | 31/135 [00:34<01:45,  1.01s/it] 24%|██▎       | 32/135 [00:35<01:47,  1.05s/it] 24%|██▍       | 33/135 [00:36<01:46,  1.05s/it] 25%|██▌       | 34/135 [00:37<01:50,  1.09s/it] 26%|██▌       | 35/135 [00:38<01:49,  1.10s/it] 27%|██▋       | 36/135 [00:39<01:40,  1.01s/it] 27%|██▋       | 37/135 [00:40<01:39,  1.02s/it] 28%|██▊       | 38/135 [00:41<01:39,  1.03s/it] 29%|██▉       | 39/135 [00:42<01:38,  1.03s/it] 30%|██▉       | 40/135 [00:43<01:39,  1.05s/it] 30%|███       | 41/135 [00:45<01:44,  1.11s/it] 31%|███       | 42/135 [00:46<01:43,  1.11s/it] 32%|███▏      | 43/135 [00:47<01:39,  1.08s/it] 33%|███▎      | 44/135 [00:48<01:38,  1.09s/it] 33%|███▎      | 45/135 [00:49<01:36,  1.07s/it] 34%|███▍      | 46/135 [00:50<01:34,  1.06s/it] 35%|███▍      | 47/135 [00:51<01:29,  1.01s/it] 36%|███▌      | 48/135 [00:52<01:28,  1.01s/it] 36%|███▋      | 49/135 [00:53<01:29,  1.04s/it] 37%|███▋      | 50/135 [00:54<01:34,  1.12s/it] 38%|███▊      | 51/135 [00:56<01:37,  1.16s/it] 39%|███▊      | 52/135 [00:56<01:30,  1.09s/it] 39%|███▉      | 53/135 [00:57<01:25,  1.05s/it] 40%|████      | 54/135 [00:58<01:22,  1.01s/it] 41%|████      | 55/135 [00:59<01:21,  1.02s/it] 41%|████▏     | 56/135 [01:01<01:26,  1.09s/it] 42%|████▏     | 57/135 [01:02<01:21,  1.05s/it] 43%|████▎     | 58/135 [01:02<01:16,  1.01it/s] 44%|████▎     | 59/135 [01:03<01:13,  1.04it/s] 44%|████▍     | 60/135 [01:05<01:16,  1.03s/it] 45%|████▌     | 61/135 [01:06<01:17,  1.05s/it] 46%|████▌     | 62/135 [01:07<01:22,  1.13s/it] 47%|████▋     | 63/135 [01:08<01:19,  1.10s/it] 47%|████▋     | 64/135 [01:09<01:14,  1.05s/it] 48%|████▊     | 65/135 [01:10<01:08,  1.02it/s] 49%|████▉     | 66/135 [01:11<01:13,  1.07s/it] 50%|████▉     | 67/135 [01:12<01:16,  1.12s/it] 50%|█████     | 68/135 [01:14<01:17,  1.16s/it] 51%|█████     | 69/135 [01:15<01:13,  1.12s/it] 52%|█████▏    | 70/135 [01:16<01:09,  1.07s/it] 53%|█████▎    | 71/135 [01:16<01:06,  1.03s/it] 53%|█████▎    | 72/135 [01:18<01:06,  1.05s/it] 54%|█████▍    | 73/135 [01:19<01:04,  1.04s/it] 55%|█████▍    | 74/135 [01:20<01:01,  1.01s/it] 56%|█████▌    | 75/135 [01:21<01:00,  1.01s/it] 56%|█████▋    | 76/135 [01:22<01:02,  1.06s/it] 57%|█████▋    | 77/135 [01:23<01:02,  1.07s/it] 58%|█████▊    | 78/135 [01:24<01:00,  1.06s/it] 59%|█████▊    | 79/135 [01:25<00:57,  1.02s/it] 59%|█████▉    | 80/135 [01:26<00:54,  1.00it/s] 60%|██████    | 81/135 [01:27<00:58,  1.08s/it] 61%|██████    | 82/135 [01:28<00:55,  1.05s/it] 61%|██████▏   | 83/135 [01:29<00:53,  1.03s/it] 62%|██████▏   | 84/135 [01:30<00:50,  1.01it/s] 63%|██████▎   | 85/135 [01:31<00:55,  1.12s/it] 64%|██████▎   | 86/135 [01:32<00:55,  1.14s/it] 64%|██████▍   | 87/135 [01:33<00:52,  1.09s/it] 65%|██████▌   | 88/135 [01:35<00:51,  1.10s/it] 66%|██████▌   | 89/135 [01:36<00:50,  1.09s/it] 67%|██████▋   | 90/135 [01:37<00:47,  1.05s/it] 67%|██████▋   | 91/135 [01:38<00:49,  1.13s/it] 68%|██████▊   | 92/135 [01:39<00:46,  1.07s/it] 69%|██████▉   | 93/135 [01:40<00:45,  1.09s/it] 70%|██████▉   | 94/135 [01:41<00:44,  1.09s/it] 70%|███████   | 95/135 [01:42<00:42,  1.07s/it] 71%|███████   | 96/135 [01:43<00:45,  1.16s/it] 72%|███████▏  | 97/135 [01:44<00:41,  1.10s/it] 73%|███████▎  | 98/135 [01:45<00:39,  1.08s/it] 73%|███████▎  | 99/135 [01:47<00:38,  1.08s/it] 74%|███████▍  | 100/135 [01:48<00:40,  1.16s/it] 75%|███████▍  | 101/135 [01:49<00:37,  1.09s/it] 76%|███████▌  | 102/135 [01:50<00:36,  1.12s/it] 76%|███████▋  | 103/135 [01:51<00:33,  1.03s/it] 77%|███████▋  | 104/135 [01:52<00:32,  1.06s/it] 78%|███████▊  | 105/135 [01:53<00:32,  1.08s/it] 79%|███████▊  | 106/135 [01:54<00:31,  1.08s/it] 79%|███████▉  | 107/135 [01:55<00:31,  1.11s/it] 80%|████████  | 108/135 [01:56<00:28,  1.06s/it] 81%|████████  | 109/135 [01:57<00:27,  1.07s/it] 81%|████████▏ | 110/135 [01:58<00:27,  1.08s/it] 82%|████████▏ | 111/135 [01:59<00:25,  1.06s/it] 83%|████████▎ | 112/135 [02:00<00:22,  1.01it/s] 84%|████████▎ | 113/135 [02:01<00:21,  1.04it/s] 84%|████████▍ | 114/135 [02:02<00:21,  1.01s/it] 85%|████████▌ | 115/135 [02:03<00:20,  1.04s/it] 86%|████████▌ | 116/135 [02:05<00:20,  1.06s/it] 87%|████████▋ | 117/135 [02:05<00:18,  1.02s/it] 87%|████████▋ | 118/135 [02:07<00:17,  1.04s/it] 88%|████████▊ | 119/135 [02:08<00:17,  1.08s/it] 89%|████████▉ | 120/135 [02:09<00:15,  1.01s/it] 90%|████████▉ | 121/135 [02:10<00:14,  1.02s/it] 90%|█████████ | 122/135 [02:11<00:13,  1.05s/it] 91%|█████████ | 123/135 [02:12<00:12,  1.07s/it] 92%|█████████▏| 124/135 [02:13<00:11,  1.06s/it] 93%|█████████▎| 125/135 [02:14<00:11,  1.11s/it] 93%|█████████▎| 126/135 [02:15<00:09,  1.11s/it] 94%|█████████▍| 127/135 [02:17<00:09,  1.16s/it] 95%|█████████▍| 128/135 [02:18<00:07,  1.11s/it] 96%|█████████▌| 129/135 [02:19<00:06,  1.09s/it] 96%|█████████▋| 130/135 [02:20<00:05,  1.20s/it] 97%|█████████▋| 131/135 [02:21<00:04,  1.12s/it] 98%|█████████▊| 132/135 [02:22<00:03,  1.14s/it] 99%|█████████▊| 133/135 [02:23<00:02,  1.11s/it] 99%|█████████▉| 134/135 [02:24<00:01,  1.05s/it]100%|██████████| 135/135 [02:24<00:00,  1.18it/s]100%|██████████| 135/135 [02:24<00:00,  1.07s/it]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
ndcg_cut_10 0.5419

JOB STATISTICS
==============
Job ID: 4202495
Cluster: snellius
User/Group: drau/drau
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 03:18:36 core-walltime
Job Wall-clock time: 00:11:02
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 120.00 GB (120.00 GB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.

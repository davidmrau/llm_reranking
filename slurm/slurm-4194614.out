
Mon Oct 16 22:06:19 2023       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A100-SXM4-40GB          On  | 00000000:31:00.0 Off |                  Off |
| N/A   30C    P0              50W / 400W |      4MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:13<00:13, 13.96s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:18<00:00,  8.70s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:18<00:00,  9.49s/it]
Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.
You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32001. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc
trec_dl20
class_reranking_llama/beir_bm25_runs_top100_trec_dl20_meta-llama_Llama-2-7b-hf
DatasetDict({
    queries: Dataset({
        features: ['_id', 'text', 'title'],
        num_rows: 54
    })
    corpus: Dataset({
        features: ['_id', 'text', 'title'],
        num_rows: 10446
    })
}) queries
  0%|          | 0/54 [00:00<?, ?it/s]100%|██████████| 54/54 [00:00<00:00, 32058.37it/s]
DatasetDict({
    queries: Dataset({
        features: ['_id', 'text', 'title'],
        num_rows: 54
    })
    corpus: Dataset({
        features: ['_id', 'text', 'title'],
        num_rows: 10446
    })
}) corpus
  0%|          | 0/10446 [00:00<?, ?it/s] 33%|███▎      | 3397/10446 [00:00<00:00, 33965.25it/s] 66%|██████▌   | 6899/10446 [00:00<00:00, 34585.09it/s]100%|█████████▉| 10411/10446 [00:00<00:00, 34827.23it/s]100%|██████████| 10446/10446 [00:00<00:00, 34677.33it/s]
  0%|          | 0/169 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/gpfs/home3/drau/llm_rankers/venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/gpfs/home3/drau/llm_rankers/venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
  1%|          | 1/169 [00:02<05:38,  2.01s/it]  1%|          | 2/169 [00:02<03:12,  1.15s/it]  2%|▏         | 3/169 [00:03<02:27,  1.12it/s]  2%|▏         | 4/169 [00:04<02:25,  1.14it/s]  3%|▎         | 5/169 [00:04<02:19,  1.17it/s]  4%|▎         | 6/169 [00:05<02:21,  1.15it/s]  4%|▍         | 7/169 [00:06<02:09,  1.25it/s]  5%|▍         | 8/169 [00:07<02:06,  1.27it/s]  5%|▌         | 9/169 [00:07<02:00,  1.33it/s]  6%|▌         | 10/169 [00:08<01:58,  1.34it/s]  7%|▋         | 11/169 [00:09<01:57,  1.35it/s]  7%|▋         | 12/169 [00:09<01:48,  1.44it/s]  8%|▊         | 13/169 [00:10<01:42,  1.52it/s]  8%|▊         | 14/169 [00:10<01:31,  1.70it/s]  9%|▉         | 15/169 [00:11<01:29,  1.72it/s]  9%|▉         | 16/169 [00:11<01:27,  1.75it/s] 10%|█         | 17/169 [00:12<01:29,  1.70it/s] 11%|█         | 18/169 [00:13<01:27,  1.73it/s] 11%|█         | 19/169 [00:13<01:27,  1.72it/s] 12%|█▏        | 20/169 [00:14<01:44,  1.42it/s] 12%|█▏        | 21/169 [00:15<01:42,  1.45it/s] 13%|█▎        | 22/169 [00:16<01:45,  1.40it/s] 14%|█▎        | 23/169 [00:16<01:41,  1.44it/s] 14%|█▍        | 24/169 [00:17<01:38,  1.47it/s] 15%|█▍        | 25/169 [00:18<01:36,  1.49it/s] 15%|█▌        | 26/169 [00:18<01:33,  1.53it/s] 16%|█▌        | 27/169 [00:19<01:34,  1.51it/s] 17%|█▋        | 28/169 [00:19<01:27,  1.61it/s] 17%|█▋        | 29/169 [00:20<01:27,  1.60it/s] 18%|█▊        | 30/169 [00:21<01:28,  1.58it/s] 18%|█▊        | 31/169 [00:21<01:26,  1.59it/s] 19%|█▉        | 32/169 [00:22<01:26,  1.58it/s] 20%|█▉        | 33/169 [00:23<01:27,  1.56it/s] 20%|██        | 34/169 [00:23<01:27,  1.55it/s] 21%|██        | 35/169 [00:24<01:28,  1.52it/s] 21%|██▏       | 36/169 [00:25<01:22,  1.61it/s] 22%|██▏       | 37/169 [00:25<01:19,  1.65it/s] 22%|██▏       | 38/169 [00:26<01:21,  1.61it/s] 23%|██▎       | 39/169 [00:26<01:21,  1.59it/s] 24%|██▎       | 40/169 [00:27<01:22,  1.56it/s] 24%|██▍       | 41/169 [00:28<01:22,  1.55it/s] 25%|██▍       | 42/169 [00:28<01:21,  1.56it/s] 25%|██▌       | 43/169 [00:29<01:21,  1.55it/s] 26%|██▌       | 44/169 [00:30<01:31,  1.36it/s] 27%|██▋       | 45/169 [00:31<01:27,  1.41it/s] 27%|██▋       | 46/169 [00:31<01:24,  1.45it/s] 28%|██▊       | 47/169 [00:32<01:30,  1.35it/s] 28%|██▊       | 48/169 [00:33<01:21,  1.49it/s] 29%|██▉       | 49/169 [00:33<01:22,  1.45it/s] 30%|██▉       | 50/169 [00:34<01:20,  1.48it/s] 30%|███       | 51/169 [00:35<01:15,  1.56it/s] 31%|███       | 52/169 [00:35<01:17,  1.51it/s] 31%|███▏      | 53/169 [00:36<01:16,  1.51it/s] 32%|███▏      | 54/169 [00:37<01:13,  1.56it/s] 33%|███▎      | 55/169 [00:37<01:14,  1.52it/s] 33%|███▎      | 56/169 [00:38<01:17,  1.46it/s] 34%|███▎      | 57/169 [00:39<01:16,  1.47it/s] 34%|███▍      | 58/169 [00:39<01:15,  1.47it/s] 35%|███▍      | 59/169 [00:40<01:12,  1.51it/s] 36%|███▌      | 60/169 [00:41<01:13,  1.48it/s] 36%|███▌      | 61/169 [00:41<01:13,  1.46it/s] 37%|███▋      | 62/169 [00:42<01:08,  1.56it/s] 37%|███▋      | 63/169 [00:43<01:07,  1.57it/s] 38%|███▊      | 64/169 [00:43<01:07,  1.55it/s] 38%|███▊      | 65/169 [00:44<01:08,  1.51it/s] 39%|███▉      | 66/169 [00:45<01:07,  1.52it/s] 40%|███▉      | 67/169 [00:45<01:04,  1.59it/s] 40%|████      | 68/169 [00:46<01:01,  1.63it/s] 41%|████      | 69/169 [00:46<01:04,  1.56it/s] 41%|████▏     | 70/169 [00:47<00:59,  1.66it/s] 42%|████▏     | 71/169 [00:47<00:56,  1.74it/s] 43%|████▎     | 72/169 [00:48<00:53,  1.80it/s] 43%|████▎     | 73/169 [00:48<00:54,  1.77it/s] 44%|████▍     | 74/169 [00:49<00:54,  1.74it/s] 44%|████▍     | 75/169 [00:50<00:52,  1.79it/s] 45%|████▍     | 76/169 [00:50<00:55,  1.69it/s] 46%|████▌     | 77/169 [00:51<00:56,  1.64it/s] 46%|████▌     | 78/169 [00:52<00:59,  1.53it/s] 47%|████▋     | 79/169 [00:52<01:01,  1.45it/s] 47%|████▋     | 80/169 [00:53<00:59,  1.48it/s] 48%|████▊     | 81/169 [00:54<00:58,  1.50it/s] 49%|████▊     | 82/169 [00:55<01:01,  1.42it/s] 49%|████▉     | 83/169 [00:55<01:00,  1.42it/s] 50%|████▉     | 84/169 [00:56<00:59,  1.42it/s] 50%|█████     | 85/169 [00:57<00:57,  1.47it/s] 51%|█████     | 86/169 [00:57<00:57,  1.43it/s] 51%|█████▏    | 87/169 [00:58<00:56,  1.45it/s] 52%|█████▏    | 88/169 [00:59<00:53,  1.50it/s] 53%|█████▎    | 89/169 [00:59<00:51,  1.54it/s] 53%|█████▎    | 90/169 [01:00<00:50,  1.57it/s] 54%|█████▍    | 91/169 [01:01<00:53,  1.45it/s] 54%|█████▍    | 92/169 [01:01<00:54,  1.41it/s] 55%|█████▌    | 93/169 [01:02<00:52,  1.44it/s] 56%|█████▌    | 94/169 [01:03<00:50,  1.50it/s] 56%|█████▌    | 95/169 [01:03<00:52,  1.40it/s] 57%|█████▋    | 96/169 [01:04<00:49,  1.47it/s] 57%|█████▋    | 97/169 [01:05<00:46,  1.54it/s] 58%|█████▊    | 98/169 [01:05<00:47,  1.49it/s] 59%|█████▊    | 99/169 [01:06<00:44,  1.56it/s] 59%|█████▉    | 100/169 [01:07<00:43,  1.59it/s] 60%|█████▉    | 101/169 [01:07<00:40,  1.69it/s] 60%|██████    | 102/169 [01:08<00:39,  1.71it/s] 61%|██████    | 103/169 [01:08<00:39,  1.66it/s] 62%|██████▏   | 104/169 [01:09<00:42,  1.52it/s] 62%|██████▏   | 105/169 [01:10<00:42,  1.52it/s] 63%|██████▎   | 106/169 [01:10<00:41,  1.53it/s] 63%|██████▎   | 107/169 [01:11<00:41,  1.48it/s] 64%|██████▍   | 108/169 [01:12<00:39,  1.55it/s] 64%|██████▍   | 109/169 [01:12<00:37,  1.58it/s] 65%|██████▌   | 110/169 [01:13<00:37,  1.58it/s] 66%|██████▌   | 111/169 [01:13<00:35,  1.66it/s] 66%|██████▋   | 112/169 [01:14<00:36,  1.56it/s] 67%|██████▋   | 113/169 [01:15<00:34,  1.62it/s] 67%|██████▋   | 114/169 [01:15<00:33,  1.63it/s] 68%|██████▊   | 115/169 [01:16<00:31,  1.71it/s] 69%|██████▊   | 116/169 [01:16<00:29,  1.78it/s] 69%|██████▉   | 117/169 [01:17<00:30,  1.69it/s] 70%|██████▉   | 118/169 [01:18<00:29,  1.75it/s] 70%|███████   | 119/169 [01:18<00:28,  1.74it/s] 71%|███████   | 120/169 [01:19<00:26,  1.83it/s] 72%|███████▏  | 121/169 [01:19<00:28,  1.68it/s] 72%|███████▏  | 122/169 [01:20<00:29,  1.59it/s] 73%|███████▎  | 123/169 [01:21<00:29,  1.55it/s] 73%|███████▎  | 124/169 [01:21<00:29,  1.53it/s] 74%|███████▍  | 125/169 [01:22<00:28,  1.56it/s] 75%|███████▍  | 126/169 [01:23<00:26,  1.63it/s] 75%|███████▌  | 127/169 [01:23<00:25,  1.67it/s] 76%|███████▌  | 128/169 [01:24<00:23,  1.73it/s] 76%|███████▋  | 129/169 [01:24<00:23,  1.74it/s] 77%|███████▋  | 130/169 [01:25<00:23,  1.67it/s] 78%|███████▊  | 131/169 [01:25<00:21,  1.77it/s] 78%|███████▊  | 132/169 [01:26<00:21,  1.76it/s] 79%|███████▊  | 133/169 [01:27<00:21,  1.65it/s] 79%|███████▉  | 134/169 [01:27<00:20,  1.71it/s] 80%|███████▉  | 135/169 [01:28<00:20,  1.66it/s] 80%|████████  | 136/169 [01:28<00:19,  1.71it/s] 81%|████████  | 137/169 [01:29<00:21,  1.50it/s] 82%|████████▏ | 138/169 [01:30<00:19,  1.59it/s] 82%|████████▏ | 139/169 [01:30<00:19,  1.56it/s] 83%|████████▎ | 140/169 [01:31<00:17,  1.71it/s] 83%|████████▎ | 141/169 [01:32<00:17,  1.58it/s] 84%|████████▍ | 142/169 [01:32<00:16,  1.59it/s] 85%|████████▍ | 143/169 [01:33<00:15,  1.66it/s] 85%|████████▌ | 144/169 [01:33<00:15,  1.65it/s] 86%|████████▌ | 145/169 [01:34<00:14,  1.70it/s] 86%|████████▋ | 146/169 [01:35<00:13,  1.66it/s] 87%|████████▋ | 147/169 [01:35<00:12,  1.73it/s] 88%|████████▊ | 148/169 [01:36<00:12,  1.71it/s] 88%|████████▊ | 149/169 [01:36<00:11,  1.72it/s] 89%|████████▉ | 150/169 [01:37<00:11,  1.61it/s] 89%|████████▉ | 151/169 [01:38<00:11,  1.63it/s] 90%|████████▉ | 152/169 [01:38<00:09,  1.71it/s] 91%|█████████ | 153/169 [01:39<00:09,  1.67it/s] 91%|█████████ | 154/169 [01:39<00:09,  1.55it/s] 92%|█████████▏| 155/169 [01:40<00:09,  1.54it/s] 92%|█████████▏| 156/169 [01:41<00:08,  1.56it/s] 93%|█████████▎| 157/169 [01:41<00:07,  1.53it/s] 93%|█████████▎| 158/169 [01:42<00:06,  1.58it/s] 94%|█████████▍| 159/169 [01:43<00:06,  1.65it/s] 95%|█████████▍| 160/169 [01:43<00:05,  1.57it/s] 95%|█████████▌| 161/169 [01:44<00:04,  1.68it/s] 96%|█████████▌| 162/169 [01:44<00:04,  1.70it/s] 96%|█████████▋| 163/169 [01:45<00:03,  1.74it/s] 97%|█████████▋| 164/169 [01:45<00:02,  1.79it/s] 98%|█████████▊| 165/169 [01:46<00:02,  1.83it/s] 98%|█████████▊| 166/169 [01:46<00:01,  1.81it/s] 99%|█████████▉| 167/169 [01:47<00:01,  1.77it/s] 99%|█████████▉| 168/169 [01:48<00:00,  1.77it/s]100%|██████████| 169/169 [01:48<00:00,  1.90it/s]100%|██████████| 169/169 [01:48<00:00,  1.56it/s]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
ndcg_cut_10 0.157
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:16<00:16, 16.56s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 10.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:22<00:00, 11.27s/it]
Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.
You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32001. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc
trec_dl20
class_reranking_llama/beir_bm25_runs_top100_trec_dl20_meta-llama_Llama-2-7b-chat-hf
DatasetDict({
    queries: Dataset({
        features: ['_id', 'text', 'title'],
        num_rows: 54
    })
    corpus: Dataset({
        features: ['_id', 'text', 'title'],
        num_rows: 10446
    })
}) queries
  0%|          | 0/54 [00:00<?, ?it/s]100%|██████████| 54/54 [00:00<00:00, 31815.20it/s]
DatasetDict({
    queries: Dataset({
        features: ['_id', 'text', 'title'],
        num_rows: 54
    })
    corpus: Dataset({
        features: ['_id', 'text', 'title'],
        num_rows: 10446
    })
}) corpus
  0%|          | 0/10446 [00:00<?, ?it/s] 34%|███▎      | 3517/10446 [00:00<00:00, 35167.18it/s] 68%|██████▊   | 7091/10446 [00:00<00:00, 35502.10it/s]100%|██████████| 10446/10446 [00:00<00:00, 35461.32it/s]
  0%|          | 0/169 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/gpfs/home3/drau/llm_rankers/venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/gpfs/home3/drau/llm_rankers/venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
  1%|          | 1/169 [00:01<04:08,  1.48s/it]  1%|          | 2/169 [00:02<02:37,  1.06it/s]  2%|▏         | 3/169 [00:02<02:08,  1.29it/s]  2%|▏         | 4/169 [00:03<02:14,  1.23it/s]  3%|▎         | 5/169 [00:04<02:12,  1.24it/s]  4%|▎         | 6/169 [00:05<02:17,  1.18it/s]  4%|▍         | 7/169 [00:05<02:06,  1.28it/s]  5%|▍         | 8/169 [00:06<02:04,  1.29it/s]  5%|▌         | 9/169 [00:07<01:58,  1.35it/s]  6%|▌         | 10/169 [00:08<01:58,  1.35it/s]  7%|▋         | 11/169 [00:08<01:57,  1.35it/s]  7%|▋         | 12/169 [00:09<01:48,  1.44it/s]  8%|▊         | 13/169 [00:09<01:41,  1.53it/s]  8%|▊         | 14/169 [00:10<01:31,  1.70it/s]  9%|▉         | 15/169 [00:10<01:29,  1.72it/s]  9%|▉         | 16/169 [00:11<01:27,  1.75it/s] 10%|█         | 17/169 [00:12<01:28,  1.72it/s] 11%|█         | 18/169 [00:12<01:27,  1.72it/s] 11%|█         | 19/169 [00:13<01:27,  1.71it/s] 12%|█▏        | 20/169 [00:14<01:44,  1.42it/s] 12%|█▏        | 21/169 [00:14<01:41,  1.46it/s] 13%|█▎        | 22/169 [00:15<01:45,  1.39it/s] 14%|█▎        | 23/169 [00:16<01:41,  1.43it/s] 14%|█▍        | 24/169 [00:16<01:39,  1.46it/s] 15%|█▍        | 25/169 [00:17<01:35,  1.50it/s] 15%|█▌        | 26/169 [00:18<01:33,  1.52it/s] 16%|█▌        | 27/169 [00:18<01:34,  1.50it/s] 17%|█▋        | 28/169 [00:19<01:28,  1.60it/s] 17%|█▋        | 29/169 [00:20<01:27,  1.60it/s] 18%|█▊        | 30/169 [00:20<01:28,  1.57it/s] 18%|█▊        | 31/169 [00:21<01:27,  1.58it/s] 19%|█▉        | 32/169 [00:21<01:26,  1.58it/s] 20%|█▉        | 33/169 [00:22<01:26,  1.57it/s] 20%|██        | 34/169 [00:23<01:27,  1.54it/s] 21%|██        | 35/169 [00:24<01:28,  1.51it/s] 21%|██▏       | 36/169 [00:24<01:22,  1.61it/s] 22%|██▏       | 37/169 [00:25<01:19,  1.66it/s] 22%|██▏       | 38/169 [00:25<01:21,  1.60it/s] 23%|██▎       | 39/169 [00:26<01:21,  1.59it/s] 24%|██▎       | 40/169 [00:27<01:22,  1.56it/s] 24%|██▍       | 41/169 [00:27<01:22,  1.56it/s] 25%|██▍       | 42/169 [00:28<01:21,  1.55it/s] 25%|██▌       | 43/169 [00:29<01:21,  1.54it/s] 26%|██▌       | 44/169 [00:29<01:32,  1.36it/s] 27%|██▋       | 45/169 [00:30<01:27,  1.42it/s] 27%|██▋       | 46/169 [00:31<01:25,  1.45it/s] 28%|██▊       | 47/169 [00:32<01:30,  1.34it/s] 28%|██▊       | 48/169 [00:32<01:21,  1.48it/s] 29%|██▉       | 49/169 [00:33<01:22,  1.46it/s] 30%|██▉       | 50/169 [00:34<01:20,  1.47it/s] 30%|███       | 51/169 [00:34<01:16,  1.55it/s] 31%|███       | 52/169 [00:35<01:17,  1.51it/s] 31%|███▏      | 53/169 [00:35<01:16,  1.52it/s] 32%|███▏      | 54/169 [00:36<01:14,  1.55it/s] 33%|███▎      | 55/169 [00:37<01:15,  1.51it/s] 33%|███▎      | 56/169 [00:38<01:17,  1.45it/s] 34%|███▎      | 57/169 [00:38<01:15,  1.48it/s] 34%|███▍      | 58/169 [00:39<01:16,  1.46it/s] 35%|███▍      | 59/169 [00:39<01:13,  1.51it/s] 36%|███▌      | 60/169 [00:40<01:14,  1.47it/s] 36%|███▌      | 61/169 [00:41<01:13,  1.47it/s] 37%|███▋      | 62/169 [00:41<01:09,  1.55it/s] 37%|███▋      | 63/169 [00:42<01:07,  1.57it/s] 38%|███▊      | 64/169 [00:43<01:07,  1.55it/s] 38%|███▊      | 65/169 [00:43<01:08,  1.52it/s] 39%|███▉      | 66/169 [00:44<01:08,  1.51it/s] 40%|███▉      | 67/169 [00:45<01:04,  1.59it/s] 40%|████      | 68/169 [00:45<01:02,  1.62it/s] 41%|████      | 69/169 [00:46<01:03,  1.57it/s] 41%|████▏     | 70/169 [00:46<00:59,  1.65it/s] 42%|████▏     | 71/169 [00:47<00:56,  1.73it/s] 43%|████▎     | 72/169 [00:47<00:53,  1.80it/s] 43%|████▎     | 73/169 [00:48<00:53,  1.79it/s] 44%|████▍     | 74/169 [00:49<00:54,  1.73it/s] 44%|████▍     | 75/169 [00:49<00:52,  1.78it/s] 45%|████▍     | 76/169 [00:50<00:55,  1.68it/s] 46%|████▌     | 77/169 [00:50<00:55,  1.65it/s] 46%|████▌     | 78/169 [00:51<00:59,  1.52it/s] 47%|████▋     | 79/169 [00:52<01:02,  1.45it/s] 47%|████▋     | 80/169 [00:53<01:00,  1.48it/s] 48%|████▊     | 81/169 [00:53<00:58,  1.51it/s] 49%|████▊     | 82/169 [00:54<01:01,  1.42it/s] 49%|████▉     | 83/169 [00:55<01:00,  1.42it/s] 50%|████▉     | 84/169 [00:56<01:00,  1.41it/s] 50%|█████     | 85/169 [00:56<00:56,  1.48it/s] 51%|█████     | 86/169 [00:57<00:58,  1.43it/s] 51%|█████▏    | 87/169 [00:58<00:56,  1.44it/s] 52%|█████▏    | 88/169 [00:58<00:54,  1.50it/s] 53%|█████▎    | 89/169 [00:59<00:51,  1.55it/s] 53%|█████▎    | 90/169 [00:59<00:50,  1.56it/s] 54%|█████▍    | 91/169 [01:00<00:54,  1.44it/s] 54%|█████▍    | 92/169 [01:01<00:54,  1.41it/s] 55%|█████▌    | 93/169 [01:02<00:52,  1.45it/s] 56%|█████▌    | 94/169 [01:02<00:50,  1.49it/s] 56%|█████▌    | 95/169 [01:03<00:52,  1.40it/s] 57%|█████▋    | 96/169 [01:04<00:49,  1.47it/s] 57%|█████▋    | 97/169 [01:04<00:46,  1.55it/s] 58%|█████▊    | 98/169 [01:05<00:48,  1.48it/s] 59%|█████▊    | 99/169 [01:06<00:44,  1.56it/s] 59%|█████▉    | 100/169 [01:06<00:43,  1.58it/s] 60%|█████▉    | 101/169 [01:07<00:39,  1.71it/s] 60%|██████    | 102/169 [01:07<00:39,  1.70it/s] 61%|██████    | 103/169 [01:08<00:39,  1.65it/s] 62%|██████▏   | 104/169 [01:09<00:42,  1.52it/s] 62%|██████▏   | 105/169 [01:09<00:41,  1.53it/s] 63%|██████▎   | 106/169 [01:10<00:41,  1.52it/s] 63%|██████▎   | 107/169 [01:11<00:42,  1.48it/s] 64%|██████▍   | 108/169 [01:11<00:39,  1.54it/s] 64%|██████▍   | 109/169 [01:12<00:37,  1.59it/s] 65%|██████▌   | 110/169 [01:12<00:37,  1.57it/s] 66%|██████▌   | 111/169 [01:13<00:35,  1.65it/s] 66%|██████▋   | 112/169 [01:14<00:36,  1.56it/s] 67%|██████▋   | 113/169 [01:14<00:34,  1.63it/s] 67%|██████▋   | 114/169 [01:15<00:33,  1.62it/s] 68%|██████▊   | 115/169 [01:15<00:31,  1.70it/s] 69%|██████▊   | 116/169 [01:16<00:29,  1.78it/s] 69%|██████▉   | 117/169 [01:17<00:30,  1.71it/s] 70%|██████▉   | 118/169 [01:17<00:29,  1.74it/s] 70%|███████   | 119/169 [01:18<00:28,  1.74it/s] 71%|███████   | 120/169 [01:18<00:26,  1.83it/s] 72%|███████▏  | 121/169 [01:19<00:28,  1.69it/s] 72%|███████▏  | 122/169 [01:20<00:29,  1.58it/s] 73%|███████▎  | 123/169 [01:20<00:29,  1.54it/s] 73%|███████▎  | 124/169 [01:21<00:29,  1.52it/s] 74%|███████▍  | 125/169 [01:22<00:28,  1.57it/s] 75%|███████▍  | 126/169 [01:22<00:26,  1.62it/s] 75%|███████▌  | 127/169 [01:23<00:25,  1.66it/s] 76%|███████▌  | 128/169 [01:23<00:23,  1.72it/s] 76%|███████▋  | 129/169 [01:24<00:22,  1.75it/s] 77%|███████▋  | 130/169 [01:24<00:23,  1.66it/s] 78%|███████▊  | 131/169 [01:25<00:21,  1.77it/s] 78%|███████▊  | 132/169 [01:26<00:21,  1.75it/s] 79%|███████▊  | 133/169 [01:26<00:21,  1.67it/s] 79%|███████▉  | 134/169 [01:27<00:20,  1.70it/s] 80%|███████▉  | 135/169 [01:27<00:20,  1.66it/s] 80%|████████  | 136/169 [01:28<00:19,  1.70it/s] 81%|████████  | 137/169 [01:29<00:21,  1.51it/s] 82%|████████▏ | 138/169 [01:29<00:19,  1.58it/s] 82%|████████▏ | 139/169 [01:30<00:19,  1.55it/s] 83%|████████▎ | 140/169 [01:30<00:17,  1.70it/s] 83%|████████▎ | 141/169 [01:31<00:17,  1.59it/s] 84%|████████▍ | 142/169 [01:32<00:17,  1.59it/s] 85%|████████▍ | 143/169 [01:32<00:15,  1.65it/s] 85%|████████▌ | 144/169 [01:33<00:15,  1.65it/s] 86%|████████▌ | 145/169 [01:34<00:14,  1.71it/s] 86%|████████▋ | 146/169 [01:34<00:13,  1.66it/s] 87%|████████▋ | 147/169 [01:35<00:12,  1.72it/s] 88%|████████▊ | 148/169 [01:35<00:12,  1.71it/s] 88%|████████▊ | 149/169 [01:36<00:11,  1.73it/s] 89%|████████▉ | 150/169 [01:37<00:11,  1.61it/s] 89%|████████▉ | 151/169 [01:37<00:11,  1.62it/s] 90%|████████▉ | 152/169 [01:38<00:09,  1.71it/s] 91%|█████████ | 153/169 [01:38<00:09,  1.68it/s] 91%|█████████ | 154/169 [01:39<00:09,  1.55it/s] 92%|█████████▏| 155/169 [01:40<00:09,  1.53it/s] 92%|█████████▏| 156/169 [01:40<00:08,  1.55it/s] 93%|█████████▎| 157/169 [01:41<00:07,  1.54it/s] 93%|█████████▎| 158/169 [01:42<00:06,  1.58it/s] 94%|█████████▍| 159/169 [01:42<00:06,  1.64it/s] 95%|█████████▍| 160/169 [01:43<00:05,  1.57it/s] 95%|█████████▌| 161/169 [01:43<00:04,  1.69it/s] 96%|█████████▌| 162/169 [01:44<00:04,  1.69it/s] 96%|█████████▋| 163/169 [01:44<00:03,  1.73it/s] 97%|█████████▋| 164/169 [01:45<00:02,  1.79it/s] 98%|█████████▊| 165/169 [01:46<00:02,  1.85it/s] 98%|█████████▊| 166/169 [01:46<00:01,  1.81it/s] 99%|█████████▉| 167/169 [01:47<00:01,  1.77it/s] 99%|█████████▉| 168/169 [01:47<00:00,  1.76it/s]100%|██████████| 169/169 [01:48<00:00,  1.90it/s]100%|██████████| 169/169 [01:48<00:00,  1.56it/s]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
ndcg_cut_10 0.3617
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:15<00:31, 15.91s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:31<00:15, 15.88s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:41<00:00, 13.13s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:41<00:00, 13.88s/it]
Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.
You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32001. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc
trec_dl20
class_reranking_llama/beir_bm25_runs_top100_trec_dl20_meta-llama_Llama-2-13b-hf
DatasetDict({
    queries: Dataset({
        features: ['_id', 'text', 'title'],
        num_rows: 54
    })
    corpus: Dataset({
        features: ['_id', 'text', 'title'],
        num_rows: 10446
    })
}) queries
  0%|          | 0/54 [00:00<?, ?it/s]100%|██████████| 54/54 [00:00<00:00, 31770.57it/s]
DatasetDict({
    queries: Dataset({
        features: ['_id', 'text', 'title'],
        num_rows: 54
    })
    corpus: Dataset({
        features: ['_id', 'text', 'title'],
        num_rows: 10446
    })
}) corpus
  0%|          | 0/10446 [00:00<?, ?it/s] 34%|███▍      | 3547/10446 [00:00<00:00, 35468.51it/s] 68%|██████▊   | 7127/10446 [00:00<00:00, 35659.82it/s]100%|██████████| 10446/10446 [00:00<00:00, 35668.63it/s]
  0%|          | 0/169 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/gpfs/home3/drau/llm_rankers/venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/gpfs/home3/drau/llm_rankers/venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
  1%|          | 1/169 [00:01<05:22,  1.92s/it]  1%|          | 2/169 [00:02<03:46,  1.36s/it]  2%|▏         | 3/169 [00:03<03:19,  1.20s/it]  2%|▏         | 4/169 [00:05<03:39,  1.33s/it]  3%|▎         | 5/169 [00:06<03:42,  1.36s/it]  4%|▎         | 6/169 [00:08<03:56,  1.45s/it]  4%|▍         | 7/169 [00:09<03:38,  1.35s/it]  5%|▍         | 8/169 [00:10<03:37,  1.35s/it]  5%|▌         | 9/169 [00:12<03:27,  1.29s/it]  6%|▌         | 10/169 [00:13<03:25,  1.29s/it]  7%|▋         | 11/169 [00:14<03:24,  1.29s/it]  7%|▋         | 12/169 [00:15<03:10,  1.21s/it]  8%|▊         | 13/169 [00:16<02:58,  1.14s/it]  8%|▊         | 14/169 [00:17<02:38,  1.02s/it]  9%|▉         | 15/169 [00:18<02:35,  1.01s/it]  9%|▉         | 16/169 [00:19<02:32,  1.01it/s] 10%|█         | 17/169 [00:20<02:34,  1.02s/it] 11%|█         | 18/169 [00:21<02:31,  1.00s/it] 11%|█         | 19/169 [00:22<02:31,  1.01s/it] 12%|█▏        | 20/169 [00:24<03:01,  1.22s/it] 12%|█▏        | 21/169 [00:25<02:56,  1.19s/it] 13%|█▎        | 22/169 [00:26<03:03,  1.25s/it] 14%|█▎        | 23/169 [00:27<02:57,  1.22s/it] 14%|█▍        | 24/169 [00:28<02:52,  1.19s/it] 15%|█▍        | 25/169 [00:30<02:47,  1.16s/it] 15%|█▌        | 26/169 [00:31<02:41,  1.13s/it] 16%|█▌        | 27/169 [00:32<02:43,  1.15s/it] 17%|█▋        | 28/169 [00:33<02:33,  1.09s/it] 17%|█▋        | 29/169 [00:34<02:31,  1.08s/it] 18%|█▊        | 30/169 [00:35<02:33,  1.10s/it] 18%|█▊        | 31/169 [00:36<02:31,  1.10s/it] 19%|█▉        | 32/169 [00:37<02:31,  1.10s/it] 20%|█▉        | 33/169 [00:38<02:31,  1.11s/it] 20%|██        | 34/169 [00:39<02:31,  1.12s/it] 21%|██        | 35/169 [00:41<02:34,  1.15s/it] 21%|██▏       | 36/169 [00:42<02:24,  1.09s/it] 22%|██▏       | 37/169 [00:43<02:18,  1.05s/it] 22%|██▏       | 38/169 [00:44<02:21,  1.08s/it] 23%|██▎       | 39/169 [00:45<02:22,  1.10s/it] 24%|██▎       | 40/169 [00:46<02:25,  1.13s/it] 24%|██▍       | 41/169 [00:47<02:24,  1.13s/it] 25%|██▍       | 42/169 [00:48<02:21,  1.11s/it] 25%|██▌       | 43/169 [00:49<02:21,  1.13s/it] 26%|██▌       | 44/169 [00:51<02:40,  1.28s/it] 27%|██▋       | 45/169 [00:52<02:32,  1.23s/it] 27%|██▋       | 46/169 [00:53<02:27,  1.20s/it] 28%|██▊       | 47/169 [00:55<02:38,  1.30s/it] 28%|██▊       | 48/169 [00:56<02:21,  1.17s/it] 29%|██▉       | 49/169 [00:57<02:23,  1.19s/it] 30%|██▉       | 50/169 [00:58<02:20,  1.18s/it] 30%|███       | 51/169 [00:59<02:11,  1.12s/it] 31%|███       | 52/169 [01:00<02:14,  1.15s/it] 31%|███▏      | 53/169 [01:01<02:12,  1.14s/it] 32%|███▏      | 54/169 [01:03<02:08,  1.12s/it] 33%|███▎      | 55/169 [01:04<02:10,  1.15s/it] 33%|███▎      | 56/169 [01:05<02:14,  1.19s/it] 34%|███▎      | 57/169 [01:06<02:12,  1.18s/it] 34%|███▍      | 58/169 [01:07<02:12,  1.19s/it] 35%|███▍      | 59/169 [01:08<02:06,  1.15s/it] 36%|███▌      | 60/169 [01:10<02:08,  1.18s/it] 36%|███▌      | 61/169 [01:11<02:08,  1.19s/it] 37%|███▋      | 62/169 [01:12<01:59,  1.12s/it] 37%|███▋      | 63/169 [01:13<01:57,  1.11s/it] 38%|███▊      | 64/169 [01:14<01:57,  1.12s/it] 38%|███▊      | 65/169 [01:15<01:59,  1.15s/it] 39%|███▉      | 66/169 [01:16<01:58,  1.15s/it] 40%|███▉      | 67/169 [01:17<01:51,  1.09s/it] 40%|████      | 68/169 [01:18<01:47,  1.07s/it] 41%|████      | 69/169 [01:20<01:51,  1.11s/it] 41%|████▏     | 70/169 [01:21<01:43,  1.04s/it] 42%|████▏     | 71/169 [01:21<01:37,  1.01it/s] 43%|████▎     | 72/169 [01:22<01:32,  1.05it/s] 43%|████▎     | 73/169 [01:23<01:32,  1.04it/s] 44%|████▍     | 74/169 [01:24<01:34,  1.01it/s] 44%|████▍     | 75/169 [01:25<01:30,  1.04it/s] 45%|████▍     | 76/169 [01:26<01:35,  1.03s/it] 46%|████▌     | 77/169 [01:28<01:37,  1.06s/it] 46%|████▌     | 78/169 [01:29<01:43,  1.14s/it] 47%|████▋     | 79/169 [01:30<01:48,  1.20s/it] 47%|████▋     | 80/169 [01:31<01:44,  1.18s/it] 48%|████▊     | 81/169 [01:32<01:42,  1.16s/it] 49%|████▊     | 82/169 [01:34<01:46,  1.23s/it] 49%|████▉     | 83/169 [01:35<01:45,  1.23s/it] 50%|████▉     | 84/169 [01:36<01:44,  1.23s/it] 50%|█████     | 85/169 [01:37<01:39,  1.18s/it] 51%|█████     | 86/169 [01:39<01:40,  1.21s/it] 51%|█████▏    | 87/169 [01:40<01:39,  1.21s/it] 52%|█████▏    | 88/169 [01:41<01:33,  1.16s/it] 53%|█████▎    | 89/169 [01:42<01:30,  1.13s/it] 53%|█████▎    | 90/169 [01:43<01:27,  1.10s/it] 54%|█████▍    | 91/169 [01:44<01:33,  1.20s/it] 54%|█████▍    | 92/169 [01:46<01:34,  1.23s/it] 55%|█████▌    | 93/169 [01:47<01:31,  1.20s/it] 56%|█████▌    | 94/169 [01:48<01:27,  1.16s/it] 56%|█████▌    | 95/169 [01:49<01:31,  1.24s/it] 57%|█████▋    | 96/169 [01:50<01:26,  1.18s/it] 57%|█████▋    | 97/169 [01:51<01:20,  1.12s/it] 58%|█████▊    | 98/169 [01:53<01:23,  1.17s/it] 59%|█████▊    | 99/169 [01:54<01:17,  1.11s/it] 59%|█████▉    | 100/169 [01:55<01:15,  1.09s/it] 60%|█████▉    | 101/169 [01:56<01:09,  1.02s/it] 60%|██████    | 102/169 [01:56<01:07,  1.01s/it] 61%|██████    | 103/169 [01:58<01:08,  1.04s/it] 62%|██████▏   | 104/169 [01:59<01:14,  1.14s/it] 62%|██████▏   | 105/169 [02:00<01:12,  1.14s/it] 63%|██████▎   | 106/169 [02:01<01:11,  1.14s/it] 63%|██████▎   | 107/169 [02:03<01:12,  1.18s/it] 64%|██████▍   | 108/169 [02:04<01:08,  1.13s/it] 64%|██████▍   | 109/169 [02:05<01:05,  1.10s/it] 65%|██████▌   | 110/169 [02:06<01:05,  1.11s/it] 66%|██████▌   | 111/169 [02:07<01:01,  1.06s/it] 66%|██████▋   | 112/169 [02:08<01:03,  1.12s/it] 67%|██████▋   | 113/169 [02:09<01:00,  1.07s/it] 67%|██████▋   | 114/169 [02:10<00:58,  1.06s/it] 68%|██████▊   | 115/169 [02:11<00:54,  1.01s/it] 69%|██████▊   | 116/169 [02:12<00:51,  1.03it/s] 69%|██████▉   | 117/169 [02:13<00:52,  1.02s/it] 70%|██████▉   | 118/169 [02:14<00:50,  1.01it/s] 70%|███████   | 119/169 [02:15<00:50,  1.00s/it] 71%|███████   | 120/169 [02:16<00:46,  1.06it/s] 72%|███████▏  | 121/169 [02:17<00:49,  1.03s/it] 72%|███████▏  | 122/169 [02:18<00:51,  1.09s/it] 73%|███████▎  | 123/169 [02:19<00:51,  1.12s/it] 73%|███████▎  | 124/169 [02:20<00:51,  1.15s/it] 74%|███████▍  | 125/169 [02:21<00:49,  1.12s/it] 75%|███████▍  | 126/169 [02:22<00:45,  1.07s/it] 75%|███████▌  | 127/169 [02:23<00:43,  1.04s/it] 76%|███████▌  | 128/169 [02:24<00:41,  1.01s/it] 76%|███████▋  | 129/169 [02:25<00:39,  1.00it/s] 77%|███████▋  | 130/169 [02:26<00:40,  1.04s/it] 78%|███████▊  | 131/169 [02:27<00:37,  1.02it/s] 78%|███████▊  | 132/169 [02:28<00:36,  1.01it/s] 79%|███████▊  | 133/169 [02:29<00:37,  1.05s/it] 79%|███████▉  | 134/169 [02:30<00:35,  1.02s/it] 80%|███████▉  | 135/169 [02:32<00:35,  1.05s/it] 80%|████████  | 136/169 [02:33<00:33,  1.02s/it] 81%|████████  | 137/169 [02:34<00:36,  1.15s/it] 82%|████████▏ | 138/169 [02:35<00:33,  1.10s/it] 82%|████████▏ | 139/169 [02:36<00:33,  1.12s/it] 83%|████████▎ | 140/169 [02:37<00:29,  1.02s/it] 83%|████████▎ | 141/169 [02:38<00:30,  1.10s/it] 84%|████████▍ | 142/169 [02:39<00:29,  1.08s/it] 85%|████████▍ | 143/169 [02:40<00:27,  1.04s/it] 85%|████████▌ | 144/169 [02:41<00:26,  1.05s/it] 86%|████████▌ | 145/169 [02:42<00:24,  1.02s/it] 86%|████████▋ | 146/169 [02:43<00:24,  1.05s/it] 87%|████████▋ | 147/169 [02:44<00:22,  1.01s/it] 88%|████████▊ | 148/169 [02:45<00:21,  1.02s/it] 88%|████████▊ | 149/169 [02:46<00:20,  1.01s/it] 89%|████████▉ | 150/169 [02:47<00:20,  1.08s/it] 89%|████████▉ | 151/169 [02:49<00:19,  1.07s/it] 90%|████████▉ | 152/169 [02:49<00:17,  1.01s/it] 91%|█████████ | 153/169 [02:50<00:16,  1.03s/it] 91%|█████████ | 154/169 [02:52<00:16,  1.11s/it] 92%|█████████▏| 155/169 [02:53<00:15,  1.13s/it] 92%|█████████▏| 156/169 [02:54<00:14,  1.12s/it] 93%|█████████▎| 157/169 [02:55<00:13,  1.14s/it] 93%|█████████▎| 158/169 [02:56<00:12,  1.10s/it] 94%|█████████▍| 159/169 [02:57<00:10,  1.05s/it] 95%|█████████▍| 160/169 [02:58<00:09,  1.11s/it] 95%|█████████▌| 161/169 [02:59<00:08,  1.03s/it] 96%|█████████▌| 162/169 [03:00<00:07,  1.01s/it] 96%|█████████▋| 163/169 [03:01<00:05,  1.01it/s] 97%|█████████▋| 164/169 [03:02<00:04,  1.04it/s] 98%|█████████▊| 165/169 [03:03<00:03,  1.07it/s] 98%|█████████▊| 166/169 [03:04<00:02,  1.06it/s] 99%|█████████▉| 167/169 [03:05<00:01,  1.02it/s] 99%|█████████▉| 168/169 [03:06<00:00,  1.02it/s]100%|██████████| 169/169 [03:07<00:00,  1.10it/s]100%|██████████| 169/169 [03:07<00:00,  1.11s/it]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
ndcg_cut_10 0.2555
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:13<00:27, 13.81s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:27<00:13, 13.82s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:36<00:00, 11.60s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:36<00:00, 12.19s/it]
Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.
You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32001. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc
trec_dl20
class_reranking_llama/beir_bm25_runs_top100_trec_dl20_meta-llama_Llama-2-13b-chat-hf
DatasetDict({
    queries: Dataset({
        features: ['_id', 'text', 'title'],
        num_rows: 54
    })
    corpus: Dataset({
        features: ['_id', 'text', 'title'],
        num_rows: 10446
    })
}) queries
  0%|          | 0/54 [00:00<?, ?it/s]100%|██████████| 54/54 [00:00<00:00, 31868.92it/s]
DatasetDict({
    queries: Dataset({
        features: ['_id', 'text', 'title'],
        num_rows: 54
    })
    corpus: Dataset({
        features: ['_id', 'text', 'title'],
        num_rows: 10446
    })
}) corpus
  0%|          | 0/10446 [00:00<?, ?it/s] 34%|███▍      | 3534/10446 [00:00<00:00, 35336.92it/s] 68%|██████▊   | 7100/10446 [00:00<00:00, 35521.85it/s]100%|██████████| 10446/10446 [00:00<00:00, 35499.53it/s]
  0%|          | 0/169 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/gpfs/home3/drau/llm_rankers/venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/gpfs/home3/drau/llm_rankers/venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
  1%|          | 1/169 [00:01<05:19,  1.90s/it]  1%|          | 2/169 [00:02<03:45,  1.35s/it]  2%|▏         | 3/169 [00:03<03:18,  1.20s/it]  2%|▏         | 4/169 [00:05<03:39,  1.33s/it]  3%|▎         | 5/169 [00:06<03:42,  1.36s/it]  4%|▎         | 6/169 [00:08<03:56,  1.45s/it]  4%|▍         | 7/169 [00:09<03:38,  1.35s/it]  5%|▍         | 8/169 [00:10<03:36,  1.35s/it]  5%|▌         | 9/169 [00:12<03:26,  1.29s/it]  6%|▌         | 10/169 [00:13<03:25,  1.29s/it]  7%|▋         | 11/169 [00:14<03:24,  1.29s/it]  7%|▋         | 12/169 [00:15<03:10,  1.21s/it]  8%|▊         | 13/169 [00:16<02:57,  1.14s/it]  8%|▊         | 14/169 [00:17<02:38,  1.02s/it]  9%|▉         | 15/169 [00:18<02:35,  1.01s/it]  9%|▉         | 16/169 [00:19<02:31,  1.01it/s] 10%|█         | 17/169 [00:20<02:33,  1.01s/it] 11%|█         | 18/169 [00:21<02:30,  1.00it/s] 11%|█         | 19/169 [00:22<02:31,  1.01s/it] 12%|█▏        | 20/169 [00:24<03:01,  1.22s/it] 12%|█▏        | 21/169 [00:25<02:56,  1.19s/it] 13%|█▎        | 22/169 [00:26<03:03,  1.25s/it] 14%|█▎        | 23/169 [00:27<02:57,  1.21s/it] 14%|█▍        | 24/169 [00:28<02:52,  1.19s/it] 15%|█▍        | 25/169 [00:29<02:47,  1.16s/it] 15%|█▌        | 26/169 [00:31<02:41,  1.13s/it] 16%|█▌        | 27/169 [00:32<02:43,  1.15s/it] 17%|█▋        | 28/169 [00:33<02:33,  1.09s/it] 17%|█▋        | 29/169 [00:34<02:31,  1.08s/it] 18%|█▊        | 30/169 [00:35<02:32,  1.10s/it] 18%|█▊        | 31/169 [00:36<02:31,  1.10s/it] 19%|█▉        | 32/169 [00:37<02:31,  1.10s/it] 20%|█▉        | 33/169 [00:38<02:31,  1.11s/it] 20%|██        | 34/169 [00:39<02:31,  1.12s/it] 21%|██        | 35/169 [00:41<02:33,  1.15s/it] 21%|██▏       | 36/169 [00:42<02:24,  1.08s/it] 22%|██▏       | 37/169 [00:43<02:18,  1.05s/it] 22%|██▏       | 38/169 [00:44<02:21,  1.08s/it] 23%|██▎       | 39/169 [00:45<02:22,  1.09s/it] 24%|██▎       | 40/169 [00:46<02:24,  1.12s/it] 24%|██▍       | 41/169 [00:47<02:24,  1.13s/it] 25%|██▍       | 42/169 [00:48<02:21,  1.11s/it] 25%|██▌       | 43/169 [00:49<02:21,  1.12s/it] 26%|██▌       | 44/169 [00:51<02:39,  1.28s/it] 27%|██▋       | 45/169 [00:52<02:32,  1.23s/it] 27%|██▋       | 46/169 [00:53<02:27,  1.20s/it] 28%|██▊       | 47/169 [00:55<02:38,  1.30s/it] 28%|██▊       | 48/169 [00:56<02:21,  1.17s/it] 29%|██▉       | 49/169 [00:57<02:23,  1.19s/it] 30%|██▉       | 50/169 [00:58<02:20,  1.18s/it] 30%|███       | 51/169 [00:59<02:11,  1.11s/it] 31%|███       | 52/169 [01:00<02:14,  1.15s/it] 31%|███▏      | 53/169 [01:01<02:12,  1.14s/it] 32%|███▏      | 54/169 [01:02<02:08,  1.11s/it] 33%|███▎      | 55/169 [01:04<02:10,  1.15s/it] 33%|███▎      | 56/169 [01:05<02:14,  1.19s/it] 34%|███▎      | 57/169 [01:06<02:11,  1.18s/it] 34%|███▍      | 58/169 [01:07<02:11,  1.19s/it] 35%|███▍      | 59/169 [01:08<02:06,  1.15s/it] 36%|███▌      | 60/169 [01:10<02:08,  1.18s/it] 36%|███▌      | 61/169 [01:11<02:08,  1.19s/it] 37%|███▋      | 62/169 [01:12<01:59,  1.12s/it] 37%|███▋      | 63/169 [01:13<01:57,  1.11s/it] 38%|███▊      | 64/169 [01:14<01:57,  1.12s/it] 38%|███▊      | 65/169 [01:15<01:59,  1.15s/it] 39%|███▉      | 66/169 [01:16<01:58,  1.15s/it] 40%|███▉      | 67/169 [01:17<01:51,  1.09s/it] 40%|████      | 68/169 [01:18<01:47,  1.07s/it] 41%|████      | 69/169 [01:20<01:51,  1.11s/it] 41%|████▏     | 70/169 [01:20<01:43,  1.04s/it] 42%|████▏     | 71/169 [01:21<01:37,  1.01it/s] 43%|████▎     | 72/169 [01:22<01:32,  1.04it/s] 43%|████▎     | 73/169 [01:23<01:32,  1.03it/s] 44%|████▍     | 74/169 [01:24<01:34,  1.01it/s] 44%|████▍     | 75/169 [01:25<01:30,  1.04it/s] 45%|████▍     | 76/169 [01:26<01:35,  1.03s/it] 46%|████▌     | 77/169 [01:27<01:37,  1.06s/it] 46%|████▌     | 78/169 [01:29<01:43,  1.14s/it] 47%|████▋     | 79/169 [01:30<01:48,  1.20s/it] 47%|████▋     | 80/169 [01:31<01:44,  1.18s/it] 48%|████▊     | 81/169 [01:32<01:42,  1.16s/it] 49%|████▊     | 82/169 [01:34<01:46,  1.22s/it] 49%|████▉     | 83/169 [01:35<01:45,  1.23s/it] 50%|████▉     | 84/169 [01:36<01:44,  1.23s/it] 50%|█████     | 85/169 [01:37<01:39,  1.18s/it] 51%|█████     | 86/169 [01:39<01:40,  1.21s/it] 51%|█████▏    | 87/169 [01:40<01:39,  1.21s/it] 52%|█████▏    | 88/169 [01:41<01:33,  1.16s/it] 53%|█████▎    | 89/169 [01:42<01:30,  1.13s/it] 53%|█████▎    | 90/169 [01:43<01:27,  1.10s/it] 54%|█████▍    | 91/169 [01:44<01:33,  1.20s/it] 54%|█████▍    | 92/169 [01:46<01:34,  1.23s/it] 55%|█████▌    | 93/169 [01:47<01:31,  1.20s/it] 56%|█████▌    | 94/169 [01:48<01:26,  1.16s/it] 56%|█████▌    | 95/169 [01:49<01:31,  1.24s/it] 57%|█████▋    | 96/169 [01:50<01:26,  1.18s/it] 57%|█████▋    | 97/169 [01:51<01:20,  1.12s/it] 58%|█████▊    | 98/169 [01:53<01:23,  1.17s/it] 59%|█████▊    | 99/169 [01:53<01:17,  1.11s/it] 59%|█████▉    | 100/169 [01:55<01:15,  1.09s/it] 60%|█████▉    | 101/169 [01:55<01:09,  1.02s/it] 60%|██████    | 102/169 [01:56<01:07,  1.01s/it] 61%|██████    | 103/169 [01:58<01:08,  1.04s/it] 62%|██████▏   | 104/169 [01:59<01:14,  1.14s/it] 62%|██████▏   | 105/169 [02:00<01:12,  1.14s/it] 63%|██████▎   | 106/169 [02:01<01:11,  1.14s/it] 63%|██████▎   | 107/169 [02:02<01:12,  1.18s/it] 64%|██████▍   | 108/169 [02:03<01:08,  1.13s/it] 64%|██████▍   | 109/169 [02:04<01:05,  1.10s/it] 65%|██████▌   | 110/169 [02:06<01:05,  1.11s/it] 66%|██████▌   | 111/169 [02:07<01:01,  1.05s/it] 66%|██████▋   | 112/169 [02:08<01:03,  1.12s/it] 67%|██████▋   | 113/169 [02:09<00:59,  1.07s/it] 67%|██████▋   | 114/169 [02:10<00:58,  1.06s/it] 68%|██████▊   | 115/169 [02:11<00:54,  1.01s/it] 69%|██████▊   | 116/169 [02:12<00:51,  1.03it/s] 69%|██████▉   | 117/169 [02:13<00:52,  1.02s/it] 70%|██████▉   | 118/169 [02:14<00:50,  1.01it/s] 70%|███████   | 119/169 [02:15<00:50,  1.00s/it] 71%|███████   | 120/169 [02:15<00:46,  1.06it/s] 72%|███████▏  | 121/169 [02:17<00:49,  1.03s/it] 72%|███████▏  | 122/169 [02:18<00:51,  1.09s/it] 73%|███████▎  | 123/169 [02:19<00:51,  1.12s/it] 73%|███████▎  | 124/169 [02:20<00:51,  1.15s/it] 74%|███████▍  | 125/169 [02:21<00:49,  1.12s/it] 75%|███████▍  | 126/169 [02:22<00:45,  1.07s/it] 75%|███████▌  | 127/169 [02:23<00:43,  1.04s/it] 76%|███████▌  | 128/169 [02:24<00:41,  1.01s/it] 76%|███████▋  | 129/169 [02:25<00:39,  1.00it/s] 77%|███████▋  | 130/169 [02:26<00:40,  1.04s/it] 78%|███████▊  | 131/169 [02:27<00:37,  1.02it/s] 78%|███████▊  | 132/169 [02:28<00:36,  1.01it/s] 79%|███████▊  | 133/169 [02:29<00:37,  1.05s/it] 79%|███████▉  | 134/169 [02:30<00:35,  1.02s/it] 80%|███████▉  | 135/169 [02:31<00:35,  1.05s/it] 80%|████████  | 136/169 [02:32<00:33,  1.02s/it] 81%|████████  | 137/169 [02:34<00:36,  1.15s/it] 82%|████████▏ | 138/169 [02:35<00:33,  1.09s/it] 82%|████████▏ | 139/169 [02:36<00:33,  1.12s/it] 83%|████████▎ | 140/169 [02:37<00:29,  1.02s/it] 83%|████████▎ | 141/169 [02:38<00:30,  1.10s/it] 84%|████████▍ | 142/169 [02:39<00:29,  1.08s/it] 85%|████████▍ | 143/169 [02:40<00:27,  1.04s/it] 85%|████████▌ | 144/169 [02:41<00:26,  1.05s/it] 86%|████████▌ | 145/169 [02:42<00:24,  1.02s/it] 86%|████████▋ | 146/169 [02:43<00:24,  1.05s/it] 87%|████████▋ | 147/169 [02:44<00:22,  1.01s/it] 88%|████████▊ | 148/169 [02:45<00:21,  1.02s/it] 88%|████████▊ | 149/169 [02:46<00:20,  1.01s/it] 89%|████████▉ | 150/169 [02:47<00:20,  1.08s/it] 89%|████████▉ | 151/169 [02:48<00:19,  1.07s/it] 90%|████████▉ | 152/169 [02:49<00:17,  1.01s/it] 91%|█████████ | 153/169 [02:50<00:16,  1.03s/it] 91%|█████████ | 154/169 [02:52<00:16,  1.11s/it] 92%|█████████▏| 155/169 [02:53<00:15,  1.13s/it] 92%|█████████▏| 156/169 [02:54<00:14,  1.12s/it] 93%|█████████▎| 157/169 [02:55<00:13,  1.14s/it] 93%|█████████▎| 158/169 [02:56<00:12,  1.10s/it] 94%|█████████▍| 159/169 [02:57<00:10,  1.05s/it] 95%|█████████▍| 160/169 [02:58<00:09,  1.11s/it] 95%|█████████▌| 161/169 [02:59<00:08,  1.03s/it] 96%|█████████▌| 162/169 [03:00<00:07,  1.01s/it] 96%|█████████▋| 163/169 [03:01<00:05,  1.01it/s] 97%|█████████▋| 164/169 [03:02<00:04,  1.04it/s] 98%|█████████▊| 165/169 [03:03<00:03,  1.07it/s] 98%|█████████▊| 166/169 [03:04<00:02,  1.06it/s] 99%|█████████▉| 167/169 [03:05<00:01,  1.02it/s] 99%|█████████▉| 168/169 [03:06<00:00,  1.02it/s]100%|██████████| 169/169 [03:07<00:00,  1.10it/s]100%|██████████| 169/169 [03:07<00:00,  1.11s/it]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
ndcg_cut_10 0.4289

JOB STATISTICS
==============
Job ID: 4194614
Cluster: snellius
User/Group: drau/drau
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 03:53:42 core-walltime
Job Wall-clock time: 00:12:59
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 120.00 GB (120.00 GB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.

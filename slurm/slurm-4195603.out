
Tue Oct 17 09:32:59 2023       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A100-SXM4-40GB          On  | 00000000:31:00.0 Off |                  Off |
| N/A   29C    P0              52W / 400W |      4MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA A100-SXM4-40GB          On  | 00000000:32:00.0 Off |                  Off |
| N/A   29C    P0              52W / 400W |      4MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]Loading checkpoint shards:   7%|▋         | 1/15 [00:13<03:08, 13.49s/it]Loading checkpoint shards:  13%|█▎        | 2/15 [00:26<02:54, 13.39s/it]Loading checkpoint shards:  20%|██        | 3/15 [00:40<02:40, 13.40s/it]Loading checkpoint shards:  27%|██▋       | 4/15 [00:54<02:30, 13.64s/it]Loading checkpoint shards:  33%|███▎      | 5/15 [01:07<02:15, 13.56s/it]Loading checkpoint shards:  40%|████      | 6/15 [01:20<02:01, 13.48s/it]Loading checkpoint shards:  47%|████▋     | 7/15 [01:35<01:50, 13.87s/it]Loading checkpoint shards:  53%|█████▎    | 8/15 [01:49<01:37, 13.92s/it]Loading checkpoint shards:  60%|██████    | 9/15 [02:03<01:23, 13.85s/it]Loading checkpoint shards:  67%|██████▋   | 10/15 [02:16<01:08, 13.74s/it]Loading checkpoint shards:  73%|███████▎  | 11/15 [02:30<00:55, 13.76s/it]Loading checkpoint shards:  80%|████████  | 12/15 [02:44<00:41, 13.87s/it]Loading checkpoint shards:  87%|████████▋ | 13/15 [02:58<00:27, 13.73s/it]Loading checkpoint shards:  93%|█████████▎| 14/15 [03:11<00:13, 13.70s/it]Loading checkpoint shards: 100%|██████████| 15/15 [03:12<00:00,  9.83s/it]Loading checkpoint shards: 100%|██████████| 15/15 [03:12<00:00, 12.85s/it]
Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.
You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32001. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc
trec_dl20
class_reranking_llama/beir_bm25_runs_top100_trec_dl20_meta-llama_Llama-2-70b-hf
DatasetDict({
    queries: Dataset({
        features: ['_id', 'text', 'title'],
        num_rows: 54
    })
    corpus: Dataset({
        features: ['_id', 'text', 'title'],
        num_rows: 10446
    })
}) queries
  0%|          | 0/54 [00:00<?, ?it/s]100%|██████████| 54/54 [00:00<00:00, 31637.44it/s]
DatasetDict({
    queries: Dataset({
        features: ['_id', 'text', 'title'],
        num_rows: 54
    })
    corpus: Dataset({
        features: ['_id', 'text', 'title'],
        num_rows: 10446
    })
}) corpus
  0%|          | 0/10446 [00:00<?, ?it/s] 32%|███▏      | 3391/10446 [00:00<00:00, 33901.38it/s] 66%|██████▌   | 6884/10446 [00:00<00:00, 34501.91it/s] 99%|█████████▉| 10390/10446 [00:00<00:00, 34753.97it/s]100%|██████████| 10446/10446 [00:00<00:00, 34601.99it/s]
  0%|          | 0/169 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/gpfs/home3/drau/llm_rankers/venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/gpfs/home3/drau/llm_rankers/venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
  1%|          | 1/169 [00:06<18:09,  6.48s/it]  1%|          | 2/169 [00:11<15:06,  5.43s/it]  2%|▏         | 3/169 [00:16<14:19,  5.18s/it]  2%|▏         | 4/169 [00:23<17:00,  6.18s/it]  3%|▎         | 5/169 [00:30<17:36,  6.44s/it]  4%|▎         | 6/169 [00:38<19:01,  7.00s/it]  4%|▍         | 7/169 [00:44<17:36,  6.52s/it]  5%|▍         | 8/169 [00:50<17:27,  6.50s/it]  5%|▌         | 9/169 [00:56<16:26,  6.17s/it]  6%|▌         | 10/169 [01:02<16:27,  6.21s/it]  7%|▋         | 11/169 [01:08<16:25,  6.24s/it]  7%|▋         | 12/169 [01:13<15:12,  5.81s/it]  8%|▊         | 13/169 [01:18<14:13,  5.47s/it]  8%|▊         | 14/169 [01:21<12:31,  4.85s/it]  9%|▉         | 15/169 [01:26<12:17,  4.79s/it]  9%|▉         | 16/169 [01:30<12:00,  4.71s/it] 10%|█         | 17/169 [01:35<12:08,  4.79s/it] 11%|█         | 18/169 [01:40<11:59,  4.77s/it] 11%|█         | 19/169 [01:45<12:03,  4.82s/it] 12%|█▏        | 20/169 [01:54<15:11,  6.12s/it] 12%|█▏        | 21/169 [01:59<14:28,  5.87s/it] 13%|█▎        | 22/169 [02:06<15:03,  6.15s/it] 14%|█▎        | 23/169 [02:12<14:28,  5.95s/it] 14%|█▍        | 24/169 [02:17<13:53,  5.75s/it] 15%|█▍        | 25/169 [02:22<13:19,  5.55s/it] 15%|█▌        | 26/169 [02:27<12:53,  5.41s/it] 16%|█▌        | 27/169 [02:33<13:06,  5.54s/it] 17%|█▋        | 28/169 [02:37<12:06,  5.16s/it] 17%|█▋        | 29/169 [02:42<11:58,  5.13s/it] 18%|█▊        | 30/169 [02:48<12:08,  5.24s/it] 18%|█▊        | 31/169 [02:53<12:04,  5.25s/it] 19%|█▉        | 32/169 [02:59<12:04,  5.29s/it] 20%|█▉        | 33/169 [03:04<11:58,  5.28s/it] 20%|██        | 34/169 [03:09<12:03,  5.36s/it] 21%|██        | 35/169 [03:15<12:15,  5.49s/it] 21%|██▏       | 36/169 [03:20<11:27,  5.17s/it] 22%|██▏       | 37/169 [03:24<10:56,  4.97s/it] 22%|██▏       | 38/169 [03:30<11:14,  5.15s/it] 23%|██▎       | 39/169 [03:35<11:14,  5.19s/it] 24%|██▎       | 40/169 [03:41<11:29,  5.35s/it] 24%|██▍       | 41/169 [03:46<11:22,  5.33s/it] 25%|██▍       | 42/169 [03:51<11:08,  5.27s/it] 25%|██▌       | 43/169 [03:57<11:11,  5.33s/it] 26%|██▌       | 44/169 [04:05<13:07,  6.30s/it] 27%|██▋       | 45/169 [04:10<12:20,  5.97s/it] 27%|██▋       | 46/169 [04:16<11:56,  5.82s/it] 28%|██▊       | 47/169 [04:23<12:55,  6.36s/it] 28%|██▊       | 48/169 [04:28<11:30,  5.71s/it] 29%|██▉       | 49/169 [04:34<11:38,  5.82s/it] 30%|██▉       | 50/169 [04:39<11:20,  5.72s/it] 30%|███       | 51/169 [04:44<10:38,  5.41s/it] 31%|███       | 52/169 [04:50<10:53,  5.58s/it] 31%|███▏      | 53/169 [04:55<10:38,  5.51s/it] 32%|███▏      | 54/169 [05:00<10:13,  5.34s/it] 33%|███▎      | 55/169 [05:06<10:20,  5.44s/it] 33%|███▎      | 56/169 [05:12<10:44,  5.70s/it] 34%|███▎      | 57/169 [05:17<10:28,  5.61s/it] 34%|███▍      | 58/169 [05:23<10:27,  5.65s/it] 35%|███▍      | 59/169 [05:28<10:01,  5.47s/it] 36%|███▌      | 60/169 [05:34<10:13,  5.63s/it] 36%|███▌      | 61/169 [05:40<10:12,  5.67s/it] 37%|███▋      | 62/169 [05:45<09:32,  5.35s/it] 37%|███▋      | 63/169 [05:50<09:22,  5.30s/it] 38%|███▊      | 64/169 [05:55<09:22,  5.35s/it] 38%|███▊      | 65/169 [06:01<09:32,  5.50s/it] 39%|███▉      | 66/169 [06:07<09:26,  5.50s/it] 40%|███▉      | 67/169 [06:11<08:54,  5.24s/it] 40%|████      | 68/169 [06:16<08:40,  5.15s/it] 41%|████      | 69/169 [06:22<08:55,  5.36s/it] 41%|████▏     | 70/169 [06:26<08:13,  4.98s/it] 42%|████▏     | 71/169 [06:30<07:44,  4.74s/it] 43%|████▎     | 72/169 [06:34<07:21,  4.55s/it] 43%|████▎     | 73/169 [06:39<07:20,  4.59s/it] 44%|████▍     | 74/169 [06:44<07:27,  4.71s/it] 44%|████▍     | 75/169 [06:48<07:09,  4.57s/it] 45%|████▍     | 76/169 [06:54<07:36,  4.91s/it] 46%|████▌     | 77/169 [06:59<07:40,  5.00s/it] 46%|████▌     | 78/169 [07:06<08:13,  5.42s/it] 47%|████▋     | 79/169 [07:12<08:44,  5.83s/it] 47%|████▋     | 80/169 [07:18<08:26,  5.69s/it] 48%|████▊     | 81/169 [07:23<08:09,  5.56s/it] 49%|████▊     | 82/169 [07:30<08:35,  5.92s/it] 49%|████▉     | 83/169 [07:36<08:32,  5.95s/it] 50%|████▉     | 84/169 [07:42<08:27,  5.97s/it] 50%|█████     | 85/169 [07:47<07:56,  5.67s/it] 51%|█████     | 86/169 [07:53<08:05,  5.86s/it] 51%|█████▏    | 87/169 [07:59<07:55,  5.80s/it] 52%|█████▏    | 88/169 [08:04<07:33,  5.60s/it] 53%|█████▎    | 89/169 [08:09<07:10,  5.38s/it] 53%|█████▎    | 90/169 [08:14<06:59,  5.32s/it] 54%|█████▍    | 91/169 [08:21<07:41,  5.91s/it] 54%|█████▍    | 92/169 [08:28<07:44,  6.04s/it] 55%|█████▌    | 93/169 [08:33<07:25,  5.87s/it] 56%|█████▌    | 94/169 [08:38<07:01,  5.63s/it] 56%|█████▌    | 95/169 [08:45<07:33,  6.13s/it] 57%|█████▋    | 96/169 [08:50<07:01,  5.77s/it] 57%|█████▋    | 97/169 [08:55<06:33,  5.46s/it] 58%|█████▊    | 98/169 [09:01<06:44,  5.69s/it] 59%|█████▊    | 99/169 [09:06<06:17,  5.39s/it] 59%|█████▉    | 100/169 [09:11<06:06,  5.31s/it] 60%|█████▉    | 101/169 [09:15<05:31,  4.88s/it] 60%|██████    | 102/169 [09:20<05:24,  4.84s/it] 61%|██████    | 103/169 [09:25<05:32,  5.04s/it] 62%|██████▏   | 104/169 [09:32<06:03,  5.60s/it] 62%|██████▏   | 105/169 [09:37<05:52,  5.50s/it] 63%|██████▎   | 106/169 [09:43<05:47,  5.51s/it] 63%|██████▎   | 107/169 [09:49<05:53,  5.70s/it] 64%|██████▍   | 108/169 [09:54<05:31,  5.44s/it] 64%|██████▍   | 109/169 [09:59<05:14,  5.24s/it] 65%|██████▌   | 110/169 [10:04<05:09,  5.25s/it] 66%|██████▌   | 111/169 [10:08<04:50,  5.00s/it] 66%|██████▋   | 112/169 [10:15<05:05,  5.37s/it] 67%|██████▋   | 113/169 [10:19<04:44,  5.08s/it] 67%|██████▋   | 114/169 [10:24<04:39,  5.08s/it] 68%|██████▊   | 115/169 [10:28<04:19,  4.81s/it] 69%|██████▊   | 116/169 [10:32<04:03,  4.59s/it] 69%|██████▉   | 117/169 [10:38<04:09,  4.80s/it] 70%|██████▉   | 118/169 [10:42<03:57,  4.66s/it] 70%|███████   | 119/169 [10:47<03:55,  4.71s/it] 71%|███████   | 120/169 [10:51<03:39,  4.47s/it] 72%|███████▏  | 121/169 [10:57<03:55,  4.91s/it] 72%|███████▏  | 122/169 [11:03<04:07,  5.26s/it] 73%|███████▎  | 123/169 [11:08<04:07,  5.38s/it] 73%|███████▎  | 124/169 [11:14<04:07,  5.50s/it] 74%|███████▍  | 125/169 [11:19<03:52,  5.29s/it] 75%|███████▍  | 126/169 [11:24<03:37,  5.06s/it] 75%|███████▌  | 127/169 [11:28<03:27,  4.93s/it] 76%|███████▌  | 128/169 [11:33<03:16,  4.78s/it] 76%|███████▋  | 129/169 [11:37<03:09,  4.73s/it] 77%|███████▋  | 130/169 [11:43<03:13,  4.95s/it] 78%|███████▊  | 131/169 [11:47<02:56,  4.65s/it] 78%|███████▊  | 132/169 [11:51<02:54,  4.70s/it] 79%|███████▊  | 133/169 [11:57<02:59,  4.98s/it] 79%|███████▉  | 134/169 [12:02<02:49,  4.83s/it] 80%|███████▉  | 135/169 [12:07<02:49,  4.98s/it] 80%|████████  | 136/169 [12:12<02:40,  4.87s/it] 81%|████████  | 137/169 [12:19<03:00,  5.64s/it] 82%|████████▏ | 138/169 [12:24<02:44,  5.31s/it] 82%|████████▏ | 139/169 [12:29<02:42,  5.41s/it] 83%|████████▎ | 140/169 [12:33<02:22,  4.91s/it] 83%|████████▎ | 141/169 [12:39<02:28,  5.32s/it] 84%|████████▍ | 142/169 [12:44<02:22,  5.27s/it] 85%|████████▍ | 143/169 [12:49<02:10,  5.02s/it] 85%|████████▌ | 144/169 [12:54<02:05,  5.03s/it] 86%|████████▌ | 145/169 [12:58<01:56,  4.84s/it] 86%|████████▋ | 146/169 [13:04<01:54,  4.98s/it] 87%|████████▋ | 147/169 [13:08<01:45,  4.81s/it] 88%|████████▊ | 148/169 [13:13<01:41,  4.85s/it] 88%|████████▊ | 149/169 [13:17<01:35,  4.75s/it] 89%|████████▉ | 150/169 [13:24<01:38,  5.16s/it] 89%|████████▉ | 151/169 [13:29<01:32,  5.13s/it] 90%|████████▉ | 152/169 [13:33<01:22,  4.85s/it] 91%|█████████ | 153/169 [13:38<01:18,  4.93s/it] 91%|█████████ | 154/169 [13:44<01:20,  5.37s/it] 92%|█████████▏| 155/169 [13:50<01:15,  5.39s/it] 92%|█████████▏| 156/169 [13:55<01:09,  5.34s/it] 93%|█████████▎| 157/169 [14:00<01:04,  5.40s/it] 93%|█████████▎| 158/169 [14:05<00:57,  5.21s/it] 94%|█████████▍| 159/169 [14:10<00:50,  5.02s/it] 95%|█████████▍| 160/169 [14:16<00:48,  5.35s/it] 95%|█████████▌| 161/169 [14:20<00:39,  4.92s/it] 96%|█████████▌| 162/169 [14:25<00:33,  4.85s/it] 96%|█████████▋| 163/169 [14:29<00:28,  4.75s/it] 97%|█████████▋| 164/169 [14:33<00:22,  4.59s/it] 98%|█████████▊| 165/169 [14:37<00:17,  4.44s/it] 98%|█████████▊| 166/169 [14:42<00:13,  4.54s/it] 99%|█████████▉| 167/169 [14:47<00:09,  4.66s/it] 99%|█████████▉| 168/169 [14:52<00:04,  4.69s/it]100%|██████████| 169/169 [14:55<00:00,  4.36s/it]100%|██████████| 169/169 [14:56<00:00,  5.30s/it]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
ndcg_cut_10 0.3804
Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]Loading checkpoint shards:   7%|▋         | 1/15 [00:14<03:20, 14.29s/it]Loading checkpoint shards:  13%|█▎        | 2/15 [00:28<03:02, 14.03s/it]Loading checkpoint shards:  20%|██        | 3/15 [00:41<02:46, 13.89s/it]Loading checkpoint shards:  27%|██▋       | 4/15 [00:56<02:35, 14.12s/it]Loading checkpoint shards:  33%|███▎      | 5/15 [01:09<02:19, 13.94s/it]Loading checkpoint shards:  40%|████      | 6/15 [01:23<02:04, 13.85s/it]Loading checkpoint shards:  47%|████▋     | 7/15 [01:37<01:51, 13.93s/it]Loading checkpoint shards:  53%|█████▎    | 8/15 [01:51<01:38, 14.02s/it]Loading checkpoint shards:  60%|██████    | 9/15 [02:05<01:23, 13.91s/it]Loading checkpoint shards:  67%|██████▋   | 10/15 [02:19<01:08, 13.76s/it]Loading checkpoint shards:  73%|███████▎  | 11/15 [02:32<00:54, 13.68s/it]Loading checkpoint shards:  80%|████████  | 12/15 [02:48<00:43, 14.35s/it]Loading checkpoint shards:  87%|████████▋ | 13/15 [03:06<00:30, 15.35s/it]Loading checkpoint shards:  93%|█████████▎| 14/15 [03:23<00:16, 16.03s/it]Loading checkpoint shards: 100%|██████████| 15/15 [03:24<00:00, 11.51s/it]Loading checkpoint shards: 100%|██████████| 15/15 [03:24<00:00, 13.65s/it]
Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.
You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32001. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc
trec_dl20
class_reranking_llama/beir_bm25_runs_top100_trec_dl20_meta-llama_Llama-2-70b-chat-hf
DatasetDict({
    queries: Dataset({
        features: ['_id', 'text', 'title'],
        num_rows: 54
    })
    corpus: Dataset({
        features: ['_id', 'text', 'title'],
        num_rows: 10446
    })
}) queries
  0%|          | 0/54 [00:00<?, ?it/s]100%|██████████| 54/54 [00:00<00:00, 31949.84it/s]
DatasetDict({
    queries: Dataset({
        features: ['_id', 'text', 'title'],
        num_rows: 54
    })
    corpus: Dataset({
        features: ['_id', 'text', 'title'],
        num_rows: 10446
    })
}) corpus
  0%|          | 0/10446 [00:00<?, ?it/s] 34%|███▍      | 3593/10446 [00:00<00:00, 35924.21it/s] 69%|██████▉   | 7205/10446 [00:00<00:00, 36036.06it/s]100%|██████████| 10446/10446 [00:00<00:00, 35960.47it/s]
  0%|          | 0/169 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/gpfs/home3/drau/llm_rankers/venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/gpfs/home3/drau/llm_rankers/venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
  1%|          | 1/169 [00:06<18:21,  6.56s/it]  1%|          | 2/169 [00:11<15:10,  5.45s/it]  2%|▏         | 3/169 [00:16<14:21,  5.19s/it]  2%|▏         | 4/169 [00:23<17:00,  6.19s/it]  3%|▎         | 5/169 [00:30<17:37,  6.45s/it]  4%|▎         | 6/169 [00:38<19:01,  7.00s/it]  4%|▍         | 7/169 [00:44<17:36,  6.52s/it]  5%|▍         | 8/169 [00:50<17:27,  6.51s/it]  5%|▌         | 9/169 [00:56<16:26,  6.17s/it]  6%|▌         | 10/169 [01:02<16:27,  6.21s/it]  7%|▋         | 11/169 [01:08<16:26,  6.24s/it]  7%|▋         | 12/169 [01:13<15:12,  5.81s/it]  8%|▊         | 13/169 [01:18<14:13,  5.47s/it]  8%|▊         | 14/169 [01:21<12:31,  4.85s/it]  9%|▉         | 15/169 [01:26<12:18,  4.79s/it]  9%|▉         | 16/169 [01:30<12:00,  4.71s/it] 10%|█         | 17/169 [01:35<12:08,  4.80s/it] 11%|█         | 18/169 [01:40<12:00,  4.77s/it] 11%|█         | 19/169 [01:45<12:04,  4.83s/it] 12%|█▏        | 20/169 [01:54<15:12,  6.12s/it] 12%|█▏        | 21/169 [02:00<14:28,  5.87s/it] 13%|█▎        | 22/169 [02:06<15:04,  6.15s/it] 14%|█▎        | 23/169 [02:12<14:29,  5.95s/it] 14%|█▍        | 24/169 [02:17<13:54,  5.75s/it] 15%|█▍        | 25/169 [02:22<13:20,  5.56s/it] 15%|█▌        | 26/169 [02:27<12:54,  5.41s/it] 16%|█▌        | 27/169 [02:33<13:07,  5.54s/it] 17%|█▋        | 28/169 [02:37<12:07,  5.16s/it] 17%|█▋        | 29/169 [02:43<11:59,  5.14s/it] 18%|█▊        | 30/169 [02:48<12:08,  5.24s/it] 18%|█▊        | 31/169 [02:53<12:04,  5.25s/it] 19%|█▉        | 32/169 [02:59<12:04,  5.29s/it] 20%|█▉        | 33/169 [03:04<11:58,  5.29s/it] 20%|██        | 34/169 [03:09<12:04,  5.37s/it] 21%|██        | 35/169 [03:15<12:16,  5.50s/it] 21%|██▏       | 36/169 [03:20<11:27,  5.17s/it] 22%|██▏       | 37/169 [03:24<10:57,  4.98s/it] 22%|██▏       | 38/169 [03:30<11:15,  5.16s/it] 23%|██▎       | 39/169 [03:35<11:15,  5.20s/it] 24%|██▎       | 40/169 [03:41<11:30,  5.35s/it] 24%|██▍       | 41/169 [03:46<11:22,  5.33s/it] 25%|██▍       | 42/169 [03:51<11:09,  5.27s/it] 25%|██▌       | 43/169 [03:57<11:11,  5.33s/it] 26%|██▌       | 44/169 [04:05<13:06,  6.29s/it] 27%|██▋       | 45/169 [04:10<12:20,  5.97s/it] 27%|██▋       | 46/169 [04:16<11:56,  5.82s/it] 28%|██▊       | 47/169 [04:24<12:55,  6.36s/it] 28%|██▊       | 48/169 [04:28<11:30,  5.71s/it] 29%|██▉       | 49/169 [04:34<11:38,  5.82s/it] 30%|██▉       | 50/169 [04:39<11:21,  5.72s/it] 30%|███       | 51/169 [04:44<10:38,  5.41s/it] 31%|███       | 52/169 [04:50<10:53,  5.58s/it] 31%|███▏      | 53/169 [04:55<10:38,  5.51s/it] 32%|███▏      | 54/169 [05:00<10:14,  5.34s/it] 33%|███▎      | 55/169 [05:06<10:20,  5.45s/it] 33%|███▎      | 56/169 [05:12<10:44,  5.70s/it] 34%|███▎      | 57/169 [05:18<10:28,  5.61s/it] 34%|███▍      | 58/169 [05:23<10:26,  5.64s/it] 35%|███▍      | 59/169 [05:28<10:01,  5.47s/it] 36%|███▌      | 60/169 [05:34<10:12,  5.62s/it] 36%|███▌      | 61/169 [05:40<10:12,  5.67s/it] 37%|███▋      | 62/169 [05:45<09:32,  5.35s/it] 37%|███▋      | 63/169 [05:50<09:22,  5.30s/it] 38%|███▊      | 64/169 [05:55<09:21,  5.35s/it] 38%|███▊      | 65/169 [06:01<09:32,  5.50s/it] 39%|███▉      | 66/169 [06:07<09:26,  5.50s/it] 40%|███▉      | 67/169 [06:11<08:54,  5.24s/it] 40%|████      | 68/169 [06:16<08:39,  5.14s/it] 41%|████      | 69/169 [06:22<08:55,  5.35s/it] 41%|████▏     | 70/169 [06:26<08:13,  4.98s/it] 42%|████▏     | 71/169 [06:30<07:44,  4.74s/it] 43%|████▎     | 72/169 [06:35<07:21,  4.55s/it] 43%|████▎     | 73/169 [06:39<07:19,  4.58s/it] 44%|████▍     | 74/169 [06:44<07:27,  4.71s/it] 44%|████▍     | 75/169 [06:48<07:09,  4.57s/it] 45%|████▍     | 76/169 [06:54<07:36,  4.91s/it] 46%|████▌     | 77/169 [06:59<07:40,  5.00s/it] 46%|████▌     | 78/169 [07:06<08:13,  5.42s/it] 47%|████▋     | 79/169 [07:13<08:44,  5.83s/it] 47%|████▋     | 80/169 [07:18<08:25,  5.68s/it] 48%|████▊     | 81/169 [07:23<08:09,  5.56s/it] 49%|████▊     | 82/169 [07:30<08:35,  5.92s/it] 49%|████▉     | 83/169 [07:36<08:32,  5.95s/it] 50%|████▉     | 84/169 [07:42<08:26,  5.96s/it] 50%|█████     | 85/169 [07:47<07:56,  5.67s/it] 51%|█████     | 86/169 [07:53<08:05,  5.85s/it] 51%|█████▏    | 87/169 [07:59<07:55,  5.80s/it] 52%|█████▏    | 88/169 [08:04<07:33,  5.59s/it] 53%|█████▎    | 89/169 [08:09<07:10,  5.38s/it] 53%|█████▎    | 90/169 [08:14<07:00,  5.32s/it] 54%|█████▍    | 91/169 [08:21<07:41,  5.91s/it] 54%|█████▍    | 92/169 [08:28<07:44,  6.03s/it] 55%|█████▌    | 93/169 [08:33<07:25,  5.86s/it] 56%|█████▌    | 94/169 [08:38<07:01,  5.62s/it] 56%|█████▌    | 95/169 [08:46<07:33,  6.13s/it] 57%|█████▋    | 96/169 [08:50<07:01,  5.77s/it] 57%|█████▋    | 97/169 [08:55<06:33,  5.46s/it] 58%|█████▊    | 98/169 [09:01<06:44,  5.69s/it] 59%|█████▊    | 99/169 [09:06<06:17,  5.39s/it] 59%|█████▉    | 100/169 [09:11<06:06,  5.31s/it] 60%|█████▉    | 101/169 [09:15<05:31,  4.88s/it] 60%|██████    | 102/169 [09:20<05:24,  4.84s/it] 61%|██████    | 103/169 [09:25<05:32,  5.04s/it] 62%|██████▏   | 104/169 [09:32<06:03,  5.59s/it] 62%|██████▏   | 105/169 [09:38<05:52,  5.50s/it] 63%|██████▎   | 106/169 [09:43<05:47,  5.51s/it] 63%|██████▎   | 107/169 [09:49<05:53,  5.70s/it] 64%|██████▍   | 108/169 [09:54<05:31,  5.44s/it] 64%|██████▍   | 109/169 [09:59<05:14,  5.23s/it] 65%|██████▌   | 110/169 [10:04<05:09,  5.25s/it] 66%|██████▌   | 111/169 [10:09<04:50,  5.00s/it] 66%|██████▋   | 112/169 [10:15<05:05,  5.37s/it] 67%|██████▋   | 113/169 [10:19<04:44,  5.08s/it] 67%|██████▋   | 114/169 [10:24<04:39,  5.08s/it] 68%|██████▊   | 115/169 [10:28<04:19,  4.81s/it] 69%|██████▊   | 116/169 [10:32<04:03,  4.59s/it] 69%|██████▉   | 117/169 [10:38<04:09,  4.80s/it] 70%|██████▉   | 118/169 [10:42<03:57,  4.66s/it] 70%|███████   | 119/169 [10:47<03:55,  4.71s/it] 71%|███████   | 120/169 [10:51<03:39,  4.47s/it] 72%|███████▏  | 121/169 [10:57<03:55,  4.91s/it] 72%|███████▏  | 122/169 [11:03<04:07,  5.26s/it] 73%|███████▎  | 123/169 [11:09<04:07,  5.38s/it] 73%|███████▎  | 124/169 [11:14<04:07,  5.50s/it] 74%|███████▍  | 125/169 [11:19<03:52,  5.29s/it] 75%|███████▍  | 126/169 [11:24<03:37,  5.06s/it] 75%|███████▌  | 127/169 [11:28<03:27,  4.93s/it] 76%|███████▌  | 128/169 [11:33<03:15,  4.78s/it] 76%|███████▋  | 129/169 [11:37<03:09,  4.73s/it] 77%|███████▋  | 130/169 [11:43<03:13,  4.96s/it] 78%|███████▊  | 131/169 [11:47<02:56,  4.65s/it] 78%|███████▊  | 132/169 [11:52<02:54,  4.71s/it] 79%|███████▊  | 133/169 [11:57<02:59,  4.99s/it] 79%|███████▉  | 134/169 [12:02<02:49,  4.84s/it] 80%|███████▉  | 135/169 [12:07<02:49,  4.99s/it] 80%|████████  | 136/169 [12:12<02:40,  4.87s/it] 81%|████████  | 137/169 [12:19<03:00,  5.64s/it] 82%|████████▏ | 138/169 [12:24<02:44,  5.31s/it] 82%|████████▏ | 139/169 [12:29<02:42,  5.41s/it] 83%|████████▎ | 140/169 [12:33<02:22,  4.91s/it] 83%|████████▎ | 141/169 [12:39<02:28,  5.32s/it] 84%|████████▍ | 142/169 [12:44<02:22,  5.27s/it] 85%|████████▍ | 143/169 [12:49<02:10,  5.02s/it] 85%|████████▌ | 144/169 [12:54<02:05,  5.03s/it] 86%|████████▌ | 145/169 [12:58<01:56,  4.84s/it] 86%|████████▋ | 146/169 [13:04<01:54,  4.98s/it] 87%|████████▋ | 147/169 [13:08<01:45,  4.82s/it] 88%|████████▊ | 148/169 [13:13<01:41,  4.85s/it] 88%|████████▊ | 149/169 [13:17<01:35,  4.75s/it] 89%|████████▉ | 150/169 [13:24<01:38,  5.16s/it] 89%|████████▉ | 151/169 [13:29<01:32,  5.13s/it] 90%|████████▉ | 152/169 [13:33<01:22,  4.84s/it] 91%|█████████ | 153/169 [13:38<01:18,  4.93s/it] 91%|█████████ | 154/169 [13:44<01:20,  5.37s/it] 92%|█████████▏| 155/169 [13:50<01:15,  5.38s/it] 92%|█████████▏| 156/169 [13:55<01:09,  5.33s/it] 93%|█████████▎| 157/169 [14:01<01:04,  5.39s/it] 93%|█████████▎| 158/169 [14:05<00:57,  5.21s/it] 94%|█████████▍| 159/169 [14:10<00:50,  5.02s/it] 95%|█████████▍| 160/169 [14:16<00:48,  5.35s/it] 95%|█████████▌| 161/169 [14:20<00:39,  4.92s/it] 96%|█████████▌| 162/169 [14:25<00:33,  4.85s/it] 96%|█████████▋| 163/169 [14:29<00:28,  4.75s/it] 97%|█████████▋| 164/169 [14:33<00:22,  4.59s/it] 98%|█████████▊| 165/169 [14:37<00:17,  4.44s/it] 98%|█████████▊| 166/169 [14:42<00:13,  4.54s/it] 99%|█████████▉| 167/169 [14:47<00:09,  4.66s/it] 99%|█████████▉| 168/169 [14:52<00:04,  4.68s/it]100%|██████████| 169/169 [14:55<00:00,  4.36s/it]100%|██████████| 169/169 [14:56<00:00,  5.30s/it]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
ndcg_cut_10 0.5727

JOB STATISTICS
==============
Job ID: 4195603
Cluster: snellius
User/Group: drau/drau
State: RUNNING
Nodes: 1
Cores per node: 36
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 22:26:24 core-walltime
Job Wall-clock time: 00:37:24
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 240.00 GB (240.00 GB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.

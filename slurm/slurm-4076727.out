
Tue Oct 10 12:50:07 2023       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A100-SXM4-40GB          On  | 00000000:32:00.0 Off |                    0 |
| N/A   30C    P0              50W / 400W |      4MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
- - - - - - - - - -  Quantization and Flash Attention  2.0 not used! - - - - - - - - - - 
- - - - - - - - - - Using T5 model- - - - - - - - - - 
scifact
reranking_qld/beir_qld_runs_top100_scifact_bigscience_T0_3B
DatasetDict({
    queries: Dataset({
        features: ['_id', 'title', 'text'],
        num_rows: 1109
    })
}) queries
  0%|          | 0/1109 [00:00<?, ?it/s]100%|██████████| 1109/1109 [00:00<00:00, 35847.09it/s]
DatasetDict({
    corpus: Dataset({
        features: ['_id', 'title', 'text'],
        num_rows: 5183
    })
}) corpus
  0%|          | 0/5183 [00:00<?, ?it/s] 61%|██████    | 3146/5183 [00:00<00:00, 31457.26it/s]100%|██████████| 5183/5183 [00:00<00:00, 31528.80it/s]
Traceback (most recent call last):
  File "/gpfs/home3/drau/reproducibility_query_generation/rerank_t5.py", line 183, in <module>
    rerank(dataset_name, model, tokenizer, bm25_runs, batch_size, ranking_file, hf_user)
  File "/gpfs/home3/drau/reproducibility_query_generation/rerank_t5.py", line 133, in rerank
    trec_run = load_trec_run(bm25_runs + '/' + template_run_file.format(dataset_name))
  File "/gpfs/home3/drau/reproducibility_query_generation/rerank_t5.py", line 15, in load_trec_run
    for line in open(fname):
FileNotFoundError: [Errno 2] No such file or directory: 'beir_qld_runs_top100/run.beir.qld-multifield.scifact.txt'

JOB STATISTICS
==============
Job ID: 4076727
Cluster: snellius
User/Group: drau/drau
State: RUNNING
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:00
CPU Efficiency: 0.00% of 00:15:00 core-walltime
Job Wall-clock time: 00:00:50
Memory Utilized: 0.00 MB (estimated maximum)
Memory Efficiency: 0.00% of 120.00 GB (120.00 GB/node)
WARNING: Efficiency statistics may be misleading for RUNNING jobs.

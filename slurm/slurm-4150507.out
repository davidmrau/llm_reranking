
Fri Oct 13 18:17:26 2023       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A100-SXM4-40GB          On  | 00000000:CA:00.0 Off |                  Off |
| N/A   30C    P0              47W / 400W |      4MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:36<00:36, 36.97s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:50<00:00, 23.38s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:50<00:00, 25.42s/it]
Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.
You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32001. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc
trec_dl20
class_reranking_llama/beir_bm25_runs_top100_trec_dl20_meta-llama_Llama-2-7b-hf
DatasetDict({
    queries: Dataset({
        features: ['_id', 'text', 'title'],
        num_rows: 54
    })
    corpus: Dataset({
        features: ['_id', 'text', 'title'],
        num_rows: 10446
    })
}) queries
  0%|          | 0/54 [00:00<?, ?it/s]100%|██████████| 54/54 [00:00<00:00, 31913.82it/s]
DatasetDict({
    queries: Dataset({
        features: ['_id', 'text', 'title'],
        num_rows: 54
    })
    corpus: Dataset({
        features: ['_id', 'text', 'title'],
        num_rows: 10446
    })
}) corpus
  0%|          | 0/10446 [00:00<?, ?it/s] 30%|██▉       | 3093/10446 [00:00<00:00, 30925.90it/s] 63%|██████▎   | 6622/10446 [00:00<00:00, 33489.28it/s] 97%|█████████▋| 10147/10446 [00:00<00:00, 34291.41it/s]100%|██████████| 10446/10446 [00:00<00:00, 33835.64it/s]
  0%|          | 0/72 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/gpfs/home3/drau/llm_rankers/venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/gpfs/home3/drau/llm_rankers/venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
  1%|▏         | 1/72 [00:02<03:23,  2.87s/it]  3%|▎         | 2/72 [00:04<02:44,  2.35s/it]  4%|▍         | 3/72 [00:06<02:31,  2.20s/it]  6%|▌         | 4/72 [00:08<02:16,  2.01s/it]  7%|▋         | 5/72 [00:10<02:05,  1.87s/it]  8%|▊         | 6/72 [00:11<01:50,  1.68s/it] 10%|▉         | 7/72 [00:12<01:39,  1.53s/it] 11%|█         | 8/72 [00:14<01:33,  1.46s/it] 12%|█▎        | 9/72 [00:16<01:51,  1.77s/it] 14%|█▍        | 10/72 [00:18<01:50,  1.78s/it] 15%|█▌        | 11/72 [00:19<01:43,  1.70s/it] 17%|█▋        | 12/72 [00:21<01:37,  1.63s/it] 18%|█▊        | 13/72 [00:22<01:32,  1.57s/it] 19%|█▉        | 14/72 [00:24<01:29,  1.55s/it] 21%|██        | 15/72 [00:25<01:28,  1.54s/it] 22%|██▏       | 16/72 [00:27<01:25,  1.52s/it] 24%|██▎       | 17/72 [00:28<01:23,  1.51s/it] 25%|██▌       | 18/72 [00:30<01:19,  1.48s/it] 26%|██▋       | 19/72 [00:32<01:28,  1.67s/it] 28%|██▊       | 20/72 [00:34<01:31,  1.76s/it] 29%|██▉       | 21/72 [00:35<01:27,  1.71s/it] 31%|███       | 22/72 [00:37<01:24,  1.68s/it] 32%|███▏      | 23/72 [00:38<01:18,  1.60s/it] 33%|███▎      | 24/72 [00:40<01:17,  1.62s/it] 35%|███▍      | 25/72 [00:42<01:15,  1.61s/it] 36%|███▌      | 26/72 [00:43<01:12,  1.59s/it] 38%|███▊      | 27/72 [00:45<01:09,  1.55s/it] 39%|███▉      | 28/72 [00:46<01:08,  1.55s/it] 40%|████      | 29/72 [00:48<01:05,  1.53s/it] 42%|████▏     | 30/72 [00:49<01:00,  1.45s/it] 43%|████▎     | 31/72 [00:50<00:57,  1.41s/it] 44%|████▍     | 32/72 [00:52<00:57,  1.44s/it] 46%|████▌     | 33/72 [00:53<00:59,  1.53s/it] 47%|████▋     | 34/72 [00:55<00:57,  1.50s/it] 49%|████▊     | 35/72 [00:57<00:58,  1.58s/it] 50%|█████     | 36/72 [00:58<00:57,  1.60s/it] 51%|█████▏    | 37/72 [01:00<00:54,  1.57s/it] 53%|█████▎    | 38/72 [01:01<00:51,  1.52s/it] 54%|█████▍    | 39/72 [01:03<00:53,  1.61s/it] 56%|█████▌    | 40/72 [01:05<00:54,  1.69s/it] 57%|█████▋    | 41/72 [01:06<00:49,  1.59s/it] 58%|█████▊    | 42/72 [01:08<00:48,  1.61s/it] 60%|█████▉    | 43/72 [01:09<00:45,  1.56s/it] 61%|██████    | 44/72 [01:11<00:45,  1.62s/it] 62%|██████▎   | 45/72 [01:13<00:43,  1.63s/it] 64%|██████▍   | 46/72 [01:14<00:39,  1.53s/it] 65%|██████▌   | 47/72 [01:16<00:38,  1.55s/it] 67%|██████▋   | 48/72 [01:17<00:35,  1.50s/it] 68%|██████▊   | 49/72 [01:18<00:32,  1.43s/it] 69%|██████▉   | 50/72 [01:20<00:31,  1.43s/it] 71%|███████   | 51/72 [01:21<00:30,  1.47s/it] 72%|███████▏  | 52/72 [01:23<00:30,  1.51s/it] 74%|███████▎  | 53/72 [01:24<00:28,  1.51s/it] 75%|███████▌  | 54/72 [01:26<00:25,  1.44s/it] 76%|███████▋  | 55/72 [01:27<00:24,  1.44s/it] 78%|███████▊  | 56/72 [01:29<00:23,  1.45s/it] 79%|███████▉  | 57/72 [01:30<00:21,  1.43s/it] 81%|████████  | 58/72 [01:32<00:22,  1.58s/it] 82%|████████▏ | 59/72 [01:33<00:20,  1.55s/it] 83%|████████▎ | 60/72 [01:35<00:18,  1.57s/it] 85%|████████▍ | 61/72 [01:36<00:16,  1.50s/it] 86%|████████▌ | 62/72 [01:38<00:14,  1.47s/it] 88%|████████▊ | 63/72 [01:39<00:12,  1.42s/it] 89%|████████▉ | 64/72 [01:41<00:11,  1.48s/it] 90%|█████████ | 65/72 [01:42<00:10,  1.54s/it] 92%|█████████▏| 66/72 [01:44<00:09,  1.53s/it] 93%|█████████▎| 67/72 [01:45<00:07,  1.55s/it] 94%|█████████▍| 68/72 [01:47<00:05,  1.45s/it] 96%|█████████▌| 69/72 [01:48<00:04,  1.39s/it] 97%|█████████▋| 70/72 [01:49<00:02,  1.37s/it] 99%|█████████▊| 71/72 [01:50<00:01,  1.33s/it]100%|██████████| 72/72 [01:51<00:00,  1.04it/s]100%|██████████| 72/72 [01:51<00:00,  1.54s/it]
Traceback (most recent call last):
  File "/gpfs/home3/drau/reproducibility_query_generation/rerank_class.py", line 195, in <module>
    rerank(dataset_name, model, tokenizer, bm25_runs, batch_size, ranking_file, hf_user)
  File "/gpfs/home3/drau/reproducibility_query_generation/rerank_class.py", line 173, in rerank
    eval_score = test.score(sorted_scores, q_ids)
  File "/gpfs/home3/drau/reproducibility_query_generation/metrics.py", line 92, in score
    self.write_scores(scores, qids, path)
  File "/gpfs/home3/drau/reproducibility_query_generation/metrics.py", line 84, in write_scores
    write_ranking_trec(scores, qids, f'{path}.trec')
  File "/gpfs/home3/drau/reproducibility_query_generation/metrics.py", line 20, in write_ranking_trec
    results_file = open(results_file_path, 'w')
FileNotFoundError: [Errno 2] No such file or directory: 'class_reranking_llama/beir_bm25_runs_top100_trec_dl20_meta-llama_Llama-2-7b-hf.trec'
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:39<00:39, 39.10s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:50<00:00, 23.04s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:50<00:00, 25.45s/it]
Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.
You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32001. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc
trec_dl20
class_reranking_llama/beir_bm25_runs_top100_trec_dl20_meta-llama_Llama-2-7b-chat-hf
DatasetDict({
    queries: Dataset({
        features: ['_id', 'text', 'title'],
        num_rows: 54
    })
    corpus: Dataset({
        features: ['_id', 'text', 'title'],
        num_rows: 10446
    })
}) queries
  0%|          | 0/54 [00:00<?, ?it/s]100%|██████████| 54/54 [00:00<00:00, 31828.61it/s]
DatasetDict({
    queries: Dataset({
        features: ['_id', 'text', 'title'],
        num_rows: 54
    })
    corpus: Dataset({
        features: ['_id', 'text', 'title'],
        num_rows: 10446
    })
}) corpus
  0%|          | 0/10446 [00:00<?, ?it/s] 34%|███▍      | 3535/10446 [00:00<00:00, 35346.83it/s] 68%|██████▊   | 7109/10446 [00:00<00:00, 35572.18it/s]100%|██████████| 10446/10446 [00:00<00:00, 35608.79it/s]
  0%|          | 0/72 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/gpfs/home3/drau/llm_rankers/venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/gpfs/home3/drau/llm_rankers/venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
  1%|▏         | 1/72 [00:02<02:32,  2.14s/it]  3%|▎         | 2/72 [00:04<02:22,  2.03s/it]  4%|▍         | 3/72 [00:06<02:19,  2.02s/it]  6%|▌         | 4/72 [00:07<02:09,  1.90s/it]  7%|▋         | 5/72 [00:09<02:01,  1.82s/it]  8%|▊         | 6/72 [00:10<01:47,  1.63s/it] 10%|▉         | 7/72 [00:11<01:37,  1.50s/it] 11%|█         | 8/72 [00:13<01:32,  1.44s/it] 12%|█▎        | 9/72 [00:15<01:51,  1.77s/it] 14%|█▍        | 10/72 [00:17<01:49,  1.76s/it] 15%|█▌        | 11/72 [00:19<01:43,  1.69s/it] 17%|█▋        | 12/72 [00:20<01:37,  1.63s/it] 18%|█▊        | 13/72 [00:22<01:33,  1.58s/it] 19%|█▉        | 14/72 [00:23<01:29,  1.55s/it] 21%|██        | 15/72 [00:25<01:27,  1.54s/it] 22%|██▏       | 16/72 [00:26<01:25,  1.52s/it] 24%|██▎       | 17/72 [00:28<01:23,  1.52s/it] 25%|██▌       | 18/72 [00:29<01:19,  1.48s/it] 26%|██▋       | 19/72 [00:31<01:28,  1.67s/it] 28%|██▊       | 20/72 [00:33<01:31,  1.76s/it] 29%|██▉       | 21/72 [00:35<01:27,  1.72s/it] 31%|███       | 22/72 [00:36<01:24,  1.68s/it] 32%|███▏      | 23/72 [00:38<01:18,  1.60s/it] 33%|███▎      | 24/72 [00:39<01:17,  1.62s/it] 35%|███▍      | 25/72 [00:41<01:16,  1.62s/it] 36%|███▌      | 26/72 [00:42<01:12,  1.58s/it] 38%|███▊      | 27/72 [00:44<01:09,  1.55s/it] 39%|███▉      | 28/72 [00:45<01:07,  1.54s/it] 40%|████      | 29/72 [00:47<01:06,  1.54s/it] 42%|████▏     | 30/72 [00:48<01:00,  1.45s/it] 43%|████▎     | 31/72 [00:49<00:57,  1.41s/it] 44%|████▍     | 32/72 [00:51<00:57,  1.43s/it] 46%|████▌     | 33/72 [00:53<01:00,  1.54s/it] 47%|████▋     | 34/72 [00:54<00:57,  1.50s/it] 49%|████▊     | 35/72 [00:56<00:58,  1.58s/it] 50%|█████     | 36/72 [00:58<00:57,  1.60s/it] 51%|█████▏    | 37/72 [00:59<00:55,  1.58s/it] 53%|█████▎    | 38/72 [01:00<00:51,  1.51s/it] 54%|█████▍    | 39/72 [01:02<00:53,  1.61s/it] 56%|█████▌    | 40/72 [01:04<00:53,  1.68s/it] 57%|█████▋    | 41/72 [01:06<00:49,  1.60s/it] 58%|█████▊    | 42/72 [01:07<00:48,  1.60s/it] 60%|█████▉    | 43/72 [01:09<00:45,  1.56s/it] 61%|██████    | 44/72 [01:10<00:45,  1.62s/it] 62%|██████▎   | 45/72 [01:12<00:44,  1.64s/it] 64%|██████▍   | 46/72 [01:13<00:39,  1.53s/it] 65%|██████▌   | 47/72 [01:15<00:38,  1.55s/it] 67%|██████▋   | 48/72 [01:16<00:35,  1.49s/it] 68%|██████▊   | 49/72 [01:18<00:33,  1.43s/it] 69%|██████▉   | 50/72 [01:19<00:31,  1.43s/it] 71%|███████   | 51/72 [01:21<00:30,  1.46s/it] 72%|███████▏  | 52/72 [01:22<00:30,  1.51s/it] 74%|███████▎  | 53/72 [01:24<00:28,  1.52s/it] 75%|███████▌  | 54/72 [01:25<00:25,  1.43s/it] 76%|███████▋  | 55/72 [01:26<00:24,  1.43s/it] 78%|███████▊  | 56/72 [01:28<00:23,  1.45s/it] 79%|███████▉  | 57/72 [01:29<00:21,  1.44s/it] 81%|████████  | 58/72 [01:31<00:22,  1.57s/it] 82%|████████▏ | 59/72 [01:33<00:20,  1.55s/it] 83%|████████▎ | 60/72 [01:34<00:18,  1.56s/it] 85%|████████▍ | 61/72 [01:36<00:16,  1.51s/it] 86%|████████▌ | 62/72 [01:37<00:14,  1.47s/it] 88%|████████▊ | 63/72 [01:38<00:12,  1.42s/it] 89%|████████▉ | 64/72 [01:40<00:11,  1.47s/it] 90%|█████████ | 65/72 [01:42<00:10,  1.54s/it] 92%|█████████▏| 66/72 [01:43<00:09,  1.52s/it] 93%|█████████▎| 67/72 [01:45<00:07,  1.55s/it] 94%|█████████▍| 68/72 [01:46<00:05,  1.45s/it] 96%|█████████▌| 69/72 [01:47<00:04,  1.40s/it] 97%|█████████▋| 70/72 [01:48<00:02,  1.37s/it] 99%|█████████▊| 71/72 [01:50<00:01,  1.33s/it]100%|██████████| 72/72 [01:50<00:00,  1.04it/s]100%|██████████| 72/72 [01:50<00:00,  1.53s/it]
Traceback (most recent call last):
  File "/gpfs/home3/drau/reproducibility_query_generation/rerank_class.py", line 195, in <module>
    rerank(dataset_name, model, tokenizer, bm25_runs, batch_size, ranking_file, hf_user)
  File "/gpfs/home3/drau/reproducibility_query_generation/rerank_class.py", line 173, in rerank
    eval_score = test.score(sorted_scores, q_ids)
  File "/gpfs/home3/drau/reproducibility_query_generation/metrics.py", line 92, in score
    self.write_scores(scores, qids, path)
  File "/gpfs/home3/drau/reproducibility_query_generation/metrics.py", line 84, in write_scores
    write_ranking_trec(scores, qids, f'{path}.trec')
  File "/gpfs/home3/drau/reproducibility_query_generation/metrics.py", line 20, in write_ranking_trec
    results_file = open(results_file_path, 'w')
FileNotFoundError: [Errno 2] No such file or directory: 'class_reranking_llama/beir_bm25_runs_top100_trec_dl20_meta-llama_Llama-2-7b-chat-hf.trec'
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards:  33%|███▎      | 1/3 [00:00<00:00,  9.46it/s]Downloading shards:  67%|██████▋   | 2/3 [00:00<00:00,  9.29it/s]Downloading shards: 100%|██████████| 3/3 [00:00<00:00,  9.31it/s]Downloading shards: 100%|██████████| 3/3 [00:00<00:00,  9.32it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:41<01:22, 41.01s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [01:12<00:35, 35.33s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:33<00:00, 28.72s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:33<00:00, 31.07s/it]
Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.
You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32001. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc
trec_dl20
class_reranking_llama/beir_bm25_runs_top100_trec_dl20_meta-llama_Llama-2-13b-hf
DatasetDict({
    queries: Dataset({
        features: ['_id', 'text', 'title'],
        num_rows: 54
    })
    corpus: Dataset({
        features: ['_id', 'text', 'title'],
        num_rows: 10446
    })
}) queries
  0%|          | 0/54 [00:00<?, ?it/s]100%|██████████| 54/54 [00:00<00:00, 31752.76it/s]
DatasetDict({
    queries: Dataset({
        features: ['_id', 'text', 'title'],
        num_rows: 54
    })
    corpus: Dataset({
        features: ['_id', 'text', 'title'],
        num_rows: 10446
    })
}) corpus
  0%|          | 0/10446 [00:00<?, ?it/s] 34%|███▍      | 3541/10446 [00:00<00:00, 35403.20it/s] 68%|██████▊   | 7135/10446 [00:00<00:00, 35717.26it/s]100%|██████████| 10446/10446 [00:00<00:00, 35722.57it/s]
  0%|          | 0/72 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/gpfs/home3/drau/llm_rankers/venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/gpfs/home3/drau/llm_rankers/venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
  1%|▏         | 1/72 [00:03<03:43,  3.15s/it]  1%|▏         | 1/72 [00:06<07:11,  6.07s/it]
Traceback (most recent call last):
  File "/gpfs/home3/drau/reproducibility_query_generation/rerank_class.py", line 195, in <module>
    rerank(dataset_name, model, tokenizer, bm25_runs, batch_size, ranking_file, hf_user)
  File "/gpfs/home3/drau/reproducibility_query_generation/rerank_class.py", line 159, in rerank
    scores = get_scores(model, instr_tokenized, target_tokenized)
  File "/gpfs/home3/drau/reproducibility_query_generation/rerank_class.py", line 126, in get_scores
    scores = model.generate(**instr_tokenized.to('cuda'), max_new_tokens=1, do_sample=False, output_scores=True, return_dict_in_generate=True).scores
  File "/gpfs/home3/drau/llm_rankers/venv/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/gpfs/home3/drau/llm_rankers/venv/lib/python3.9/site-packages/transformers/generation/utils.py", line 1606, in generate
    return self.greedy_search(
  File "/gpfs/home3/drau/llm_rankers/venv/lib/python3.9/site-packages/transformers/generation/utils.py", line 2454, in greedy_search
    outputs = self(
  File "/gpfs/home3/drau/llm_rankers/venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/gpfs/home3/drau/llm_rankers/venv/lib/python3.9/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
  File "/gpfs/home3/drau/llm_rankers/venv/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 1035, in forward
    outputs = self.model(
  File "/gpfs/home3/drau/llm_rankers/venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/gpfs/home3/drau/llm_rankers/venv/lib/python3.9/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
  File "/gpfs/home3/drau/llm_rankers/venv/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 922, in forward
    layer_outputs = decoder_layer(
  File "/gpfs/home3/drau/llm_rankers/venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/gpfs/home3/drau/llm_rankers/venv/lib/python3.9/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
  File "/gpfs/home3/drau/llm_rankers/venv/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 646, in forward
    hidden_states = self.mlp(hidden_states)
  File "/gpfs/home3/drau/llm_rankers/venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/gpfs/home3/drau/llm_rankers/venv/lib/python3.9/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
  File "/gpfs/home3/drau/llm_rankers/venv/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 247, in forward
    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 694.00 MiB (GPU 0; 39.39 GiB total capacity; 27.43 GiB already allocated; 370.69 MiB free; 38.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]Downloading shards:  33%|███▎      | 1/3 [00:00<00:00,  7.78it/s]Downloading shards:  67%|██████▋   | 2/3 [00:00<00:00,  4.88it/s]Downloading shards: 100%|██████████| 3/3 [00:00<00:00,  6.13it/s]Downloading shards: 100%|██████████| 3/3 [00:00<00:00,  5.99it/s]
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:40<01:20, 40.33s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [01:12<00:35, 35.32s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:32<00:00, 28.62s/it]Loading checkpoint shards: 100%|██████████| 3/3 [01:32<00:00, 30.93s/it]
Downloading (…)okenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]Downloading (…)okenizer_config.json: 100%|██████████| 1.62k/1.62k [00:00<00:00, 509kB/s]
Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.
You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32001. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc
trec_dl20
class_reranking_llama/beir_bm25_runs_top100_trec_dl20_meta-llama_Llama-2-13b-chat-hf
DatasetDict({
    queries: Dataset({
        features: ['_id', 'text', 'title'],
        num_rows: 54
    })
    corpus: Dataset({
        features: ['_id', 'text', 'title'],
        num_rows: 10446
    })
}) queries
  0%|          | 0/54 [00:00<?, ?it/s]100%|██████████| 54/54 [00:00<00:00, 31891.36it/s]
DatasetDict({
    queries: Dataset({
        features: ['_id', 'text', 'title'],
        num_rows: 54
    })
    corpus: Dataset({
        features: ['_id', 'text', 'title'],
        num_rows: 10446
    })
}) corpus
  0%|          | 0/10446 [00:00<?, ?it/s] 34%|███▎      | 3518/10446 [00:00<00:00, 35169.30it/s] 68%|██████▊   | 7054/10446 [00:00<00:00, 35280.25it/s]100%|██████████| 10446/10446 [00:00<00:00, 35291.48it/s]
  0%|          | 0/72 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
/gpfs/home3/drau/llm_rankers/venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/gpfs/home3/drau/llm_rankers/venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
  1%|▏         | 1/72 [00:03<03:38,  3.08s/it]  1%|▏         | 1/72 [00:05<07:03,  5.97s/it]
Traceback (most recent call last):
  File "/gpfs/home3/drau/reproducibility_query_generation/rerank_class.py", line 195, in <module>
    rerank(dataset_name, model, tokenizer, bm25_runs, batch_size, ranking_file, hf_user)
  File "/gpfs/home3/drau/reproducibility_query_generation/rerank_class.py", line 159, in rerank
    scores = get_scores(model, instr_tokenized, target_tokenized)
  File "/gpfs/home3/drau/reproducibility_query_generation/rerank_class.py", line 126, in get_scores
    scores = model.generate(**instr_tokenized.to('cuda'), max_new_tokens=1, do_sample=False, output_scores=True, return_dict_in_generate=True).scores
  File "/gpfs/home3/drau/llm_rankers/venv/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/gpfs/home3/drau/llm_rankers/venv/lib/python3.9/site-packages/transformers/generation/utils.py", line 1606, in generate
    return self.greedy_search(
  File "/gpfs/home3/drau/llm_rankers/venv/lib/python3.9/site-packages/transformers/generation/utils.py", line 2454, in greedy_search
    outputs = self(
  File "/gpfs/home3/drau/llm_rankers/venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/gpfs/home3/drau/llm_rankers/venv/lib/python3.9/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
  File "/gpfs/home3/drau/llm_rankers/venv/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 1035, in forward
    outputs = self.model(
  File "/gpfs/home3/drau/llm_rankers/venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/gpfs/home3/drau/llm_rankers/venv/lib/python3.9/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
  File "/gpfs/home3/drau/llm_rankers/venv/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 922, in forward
    layer_outputs = decoder_layer(
  File "/gpfs/home3/drau/llm_rankers/venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/gpfs/home3/drau/llm_rankers/venv/lib/python3.9/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
  File "/gpfs/home3/drau/llm_rankers/venv/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 646, in forward
    hidden_states = self.mlp(hidden_states)
  File "/gpfs/home3/drau/llm_rankers/venv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/gpfs/home3/drau/llm_rankers/venv/lib/python3.9/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
  File "/gpfs/home3/drau/llm_rankers/venv/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 247, in forward
    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 694.00 MiB (GPU 0; 39.39 GiB total capacity; 27.43 GiB already allocated; 370.69 MiB free; 38.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

JOB STATISTICS
==============
Job ID: 4150507
Cluster: snellius
User/Group: drau/drau
State: FAILED (exit code 1)
Nodes: 1
Cores per node: 18
CPU Utilized: 00:06:57
CPU Efficiency: 3.81% of 03:02:24 core-walltime
Job Wall-clock time: 00:10:08
Memory Utilized: 7.16 GB
Memory Efficiency: 5.97% of 120.00 GB

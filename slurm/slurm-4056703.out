
Mon Oct  9 14:49:28 2023       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A100-SXM4-40GB          On  | 00000000:31:00.0 Off |                  Off |
| N/A   30C    P0              50W / 400W |      4MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
- - - - - - - - - -  Quantization and Flash Attention  2.0 not used! - - - - - - - - - - 
- - - - - - - - - - Using T5 model- - - - - - - - - - 
cqadupstack-android
DatasetDict({
    queries: Dataset({
        features: ['_id', 'text', 'title'],
        num_rows: 699
    })
    corpus: Dataset({
        features: ['_id', 'text', 'title'],
        num_rows: 22998
    })
}) queries
  0%|          | 0/699 [00:00<?, ?it/s]100%|██████████| 699/699 [00:00<00:00, 30876.20it/s]
DatasetDict({
    queries: Dataset({
        features: ['_id', 'text', 'title'],
        num_rows: 699
    })
    corpus: Dataset({
        features: ['_id', 'text', 'title'],
        num_rows: 22998
    })
}) corpus
  0%|          | 0/22998 [00:00<?, ?it/s] 15%|█▌        | 3467/22998 [00:00<00:00, 34663.26it/s] 30%|███       | 6944/22998 [00:00<00:00, 34721.73it/s] 45%|████▌     | 10431/22998 [00:00<00:00, 34788.30it/s] 61%|██████    | 13914/22998 [00:00<00:00, 34802.45it/s] 76%|███████▌  | 17401/22998 [00:00<00:00, 34826.28it/s] 91%|█████████ | 20886/22998 [00:00<00:00, 34831.67it/s]100%|██████████| 22998/22998 [00:00<00:00, 34764.34it/s]
  0%|          | 0/365 [00:00<?, ?it/s]  0%|          | 1/365 [00:04<29:34,  4.87s/it]  1%|          | 2/365 [00:07<23:10,  3.83s/it]  1%|          | 3/365 [00:11<21:10,  3.51s/it]  1%|          | 4/365 [00:14<20:12,  3.36s/it]  1%|▏         | 5/365 [00:17<19:51,  3.31s/it]  2%|▏         | 6/365 [00:20<19:22,  3.24s/it]  2%|▏         | 7/365 [00:23<19:04,  3.20s/it]  2%|▏         | 8/365 [00:26<18:50,  3.17s/it]  2%|▏         | 9/365 [00:30<19:00,  3.20s/it]  3%|▎         | 10/365 [00:33<18:52,  3.19s/it]  3%|▎         | 11/365 [00:36<18:39,  3.16s/it]  3%|▎         | 12/365 [00:39<18:32,  3.15s/it]  4%|▎         | 13/365 [00:42<18:46,  3.20s/it]  4%|▍         | 14/365 [00:45<18:41,  3.20s/it]  4%|▍         | 15/365 [00:49<18:36,  3.19s/it]  4%|▍         | 16/365 [00:52<18:24,  3.16s/it]  5%|▍         | 17/365 [00:55<18:25,  3.18s/it]  5%|▍         | 18/365 [00:58<18:16,  3.16s/it]  5%|▌         | 19/365 [01:01<18:08,  3.15s/it]  5%|▌         | 20/365 [01:04<17:59,  3.13s/it]  6%|▌         | 21/365 [01:08<18:12,  3.18s/it]  6%|▌         | 22/365 [01:11<18:08,  3.17s/it]  6%|▋         | 23/365 [01:14<18:03,  3.17s/it]slurmstepd: error: *** JOB 4056703 ON gcn27 CANCELLED AT 2023-10-09T14:51:23 ***
slurmstepd: error: container_p_join: open failed for /slurm/4056703/.ns: No such file or directory
slurmstepd: error: container_g_join(4056703): No such file or directory

JOB STATISTICS
==============
Job ID: 4056703
Cluster: snellius
User/Group: drau/drau
State: CANCELLED (exit code 0)
Nodes: 1
Cores per node: 18
CPU Utilized: 00:00:01
CPU Efficiency: 0.04% of 00:37:12 core-walltime
Job Wall-clock time: 00:02:04
Memory Utilized: 9.73 GB
Memory Efficiency: 8.11% of 120.00 GB
